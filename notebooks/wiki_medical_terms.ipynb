{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paracetamol poisoning</td>\n",
       "      <td>Paracetamol poisoning, also known as acetamino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acromegaly</td>\n",
       "      <td>Acromegaly is a disorder that results from exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinic keratosis</td>\n",
       "      <td>Actinic keratosis (AK), sometimes called solar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congenital adrenal hyperplasia</td>\n",
       "      <td>Congenital adrenal hyperplasia (CAH) is a grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adrenocortical carcinoma</td>\n",
       "      <td>Adrenocortical carcinoma  (ACC) is an aggressi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       page_title  \\\n",
       "0           Paracetamol poisoning   \n",
       "1                      Acromegaly   \n",
       "2               Actinic keratosis   \n",
       "3  Congenital adrenal hyperplasia   \n",
       "4        Adrenocortical carcinoma   \n",
       "\n",
       "                                           page_text  \n",
       "0  Paracetamol poisoning, also known as acetamino...  \n",
       "1  Acromegaly is a disorder that results from exc...  \n",
       "2  Actinic keratosis (AK), sometimes called solar...  \n",
       "3  Congenital adrenal hyperplasia (CAH) is a grou...  \n",
       "4  Adrenocortical carcinoma  (ACC) is an aggressi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Paracetamol poisoning, also known as acetamino...\n",
       "1       Acromegaly is a disorder that results from exc...\n",
       "2       Actinic keratosis (AK), sometimes called solar...\n",
       "3       Congenital adrenal hyperplasia (CAH) is a grou...\n",
       "4       Adrenocortical carcinoma  (ACC) is an aggressi...\n",
       "                              ...                        \n",
       "7271    Gephyrophobia is the anxiety disorder or speci...\n",
       "7272    Coronary artery bypass surgery, also known as ...\n",
       "7273    Unemployment, according to the OECD (Organisat...\n",
       "7274    A surgical instrument is a tool or device for ...\n",
       "7275    Occipital neuralgia (ON) is a painful conditio...\n",
       "Name: page_text, Length: 6861, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = df.iloc[:, 1]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "articles = articles.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       paracetamol poisoning also known as acetaminop...\n",
       "1       acromegaly is a disorder that results from exc...\n",
       "2       actinic keratosis ak sometimes called solar ke...\n",
       "3       congenital adrenal hyperplasia cah is a group ...\n",
       "4       adrenocortical carcinoma acc is an aggressive ...\n",
       "                              ...                        \n",
       "7271    gephyrophobia is the anxiety disorder or speci...\n",
       "7272    coronary artery bypass surgery also known as c...\n",
       "7273    unemployment according to the oecd organisatio...\n",
       "7274    a surgical instrument is a tool or device for ...\n",
       "7275    occipital neuralgia on is a painful condition ...\n",
       "Name: page_text, Length: 6861, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1845, 23253, 321, 349, 22475, 635, 1900, 355,...\n",
       "1       [330, 398, 1533, 3400, 318, 257, 8967, 326, 24...\n",
       "2       [529, 47277, 41927, 265, 5958, 47594, 3360, 14...\n",
       "3       [36801, 268, 1287, 26999, 282, 8718, 489, 2321...\n",
       "4       [324, 918, 420, 419, 605, 28164, 6086, 697, 31...\n",
       "                              ...                        \n",
       "7271    [469, 6883, 10051, 30665, 318, 262, 9751, 8967...\n",
       "7272    [10215, 261, 560, 37646, 17286, 8185, 635, 190...\n",
       "7273    [403, 28812, 1864, 284, 262, 267, 21142, 12684...\n",
       "7274    [64, 21998, 8875, 318, 257, 2891, 393, 3335, 3...\n",
       "7275    [13966, 541, 1287, 17019, 70, 544, 319, 318, 2...\n",
       "Name: page_text, Length: 6861, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "tokenized_articles = articles.apply(lambda x: tokenizer.encode(x))\n",
    "tokenized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6861,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define maximum length for padding/truncation\n",
    "max_length = 512\n",
    "\n",
    "# Pad and truncate tokenized articles\n",
    "def pad_and_truncate(tokens, max_length):\n",
    "    if len(tokens) > max_length:\n",
    "        return tokens[:max_length]\n",
    "    return tokens + [0] * (max_length - len(tokens))\n",
    "\n",
    "# Apply padding/truncation\n",
    "tokenized_articles = tokenized_articles.apply(lambda x: pad_and_truncate(x, max_length))\n",
    "tokenized_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.protobuf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize wandb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m wandb_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set to True to enable wandb logging\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     31\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/sdk/__init__.py:25\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/sdk/artifacts/artifact.py:47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WBValue\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termerror, termlog, termwarn\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/apis/__init__.py:43\u001b[0m\n\u001b[1;32m     38\u001b[0m     _disable_ssl()\n\u001b[1;32m     41\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     46\u001b[0m reset_path()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/apis/internal.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mApi\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgql_request\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphQLSession\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m B64MD5, md5_file_b64\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m credentials, retry\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilenames\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DIFF_FNAME, METADATA_FNAME\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgitlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GitRepo\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/sdk/lib/retry.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckRetryFnType\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmailbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextCancelledError\n\u001b[1;32m     19\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# To let tests mock out the retry logic's now()/sleep() funcs, this file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# should only use these variables, not call the stdlib funcs directly.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Dict, List, Optional, Tuple\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Error\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_internal_pb2 \u001b[38;5;28;01mas\u001b[39;00m pb\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface_shared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterfaceShared\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/wandb/proto/wandb_internal_pb2.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m protobuf_version \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mprotobuf\u001b[38;5;241m.\u001b[39m__version__[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protobuf_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.protobuf'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb_log = True  # Set to True to enable wandb logging\n",
    "wandb_project = 'Attention Benchmark'\n",
    "wandb_run_name = 'Casual Attention-mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, articles, model=\"gpt2\", seq_length=512):\n",
    "        self.tokenizer = tiktoken.get_encoding(model)\n",
    "        self.vocab_size = self.tokenizer.n_vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.articles = articles.apply(self.preprocess_and_tokenize)\n",
    "        \n",
    "        self.input_ids, self.attention_masks, self.targets = self.create_sequences()\n",
    "\n",
    "    def preprocess_and_tokenize(self, text):\n",
    "        # Preprocess text\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Check for invalid token indices\n",
    "        assert all(token < self.vocab_size for token in tokens), \"Token index out of range\"\n",
    "        \n",
    "        # Pad and truncate tokens\n",
    "        if len(tokens) > self.seq_length:\n",
    "            tokens = tokens[:self.seq_length]\n",
    "        else:\n",
    "            tokens += [0] * (self.seq_length - len(tokens))\n",
    "            \n",
    "        return tokens\n",
    "\n",
    "    def create_sequences(self):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        targets = []\n",
    "        \n",
    "        for tokens in self.articles:\n",
    "            input_ids.append(tokens[:-1])  # Exclude the last token for input\n",
    "            targets.append(tokens[1:])     # Exclude the first token for target\n",
    "            attention_masks.append([1 if token != 0 else 0 for token in tokens[:-1]])\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, attention_masks, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_seq = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        \n",
    "        sample = {'input_ids': input_seq, 'targets': target_seq, 'attention_mask': attention_mask}\n",
    "        return sample\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    targets = [item['targets'] for item in batch]\n",
    "\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'input_ids': input_ids_padded, 'targets': targets_padded, 'attention_mask': attention_masks_padded }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = TextDataset(articles, model=\"gpt2\", seq_length=512)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32396,   318,   262,  ...,   393, 35961,   612],\n",
      "        [ 5657,  2411,  7721,  ...,     0,     0,     0]]) tensor([[  318,   262, 19883,  ..., 35961,   612,   318],\n",
      "        [ 2411,  7721,  4143,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    x, y, att = data['input_ids'], data['targets'], data['attention_mask']\n",
    "    print(x,y,att)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.d_k = config.n_embd // config.n_head\n",
    "        self.scale = self.d_k ** -0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.n_head, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "\n",
    "        attn_output = (attn_probs @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        attn_output = self.resid_dropout(self.out_proj(attn_output))\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None, \"Config must include vocab_size\"\n",
    "        assert config.block_size is not None, \"Config must include block_size\"\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'drop': nn.Dropout(config.dropout),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=1e-5),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.init_residuals()\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def init_residuals(self):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = logits[:, [-1], :]  # Use only the last token's logits\n",
    "            return logits, None\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}, \"Invalid model type\"\n",
    "        override_args = override_args or {}\n",
    "        assert all(k == 'dropout' for k in override_args), \"Only 'dropout' can be overridden\"\n",
    "\n",
    "        print(f\"Loading weights from pretrained model: {model_type}\")\n",
    "\n",
    "        config_args = cls.get_config_args(model_type)\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_pretrained_weights(model_type)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_args(model_type):\n",
    "        config_map = {\n",
    "            'gpt2': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'gpt2-medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'gpt2-large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'gpt2-xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
    "        }\n",
    "        config_args = config_map[model_type]\n",
    "        config_args.update({'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'dropout': 0.1})\n",
    "        return config_args\n",
    "\n",
    "    def load_pretrained_weights(self, model_type):\n",
    "        model_hf = transformers.GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd = self.state_dict()\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for k, v in sd_hf.items():\n",
    "            if k in sd:\n",
    "                if any(k.endswith(w) for w in transposed):\n",
    "                    sd[k].copy_(v.t())\n",
    "                else:\n",
    "                    sd[k].copy_(v)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        N = self.get_num_params()\n",
    "        L, H, Q, T = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.block_size\n",
    "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        flops_achieved = flops_per_iter * (1.0 / dt)\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dependencies\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import nullcontext\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per iteration will be: 491,520\n"
     ]
    }
   ],
   "source": [
    "# Default config values designed to train a GPT-2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# Data\n",
    "# data_path = 'data/openwebtext.txt'  # Path to your text file\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 12\n",
    "block_size = 1024\n",
    "\n",
    "# Model\n",
    "n_layer = 6  # Reduce the number of layers\n",
    "n_head = 8   # Reduce the number of attention heads\n",
    "n_embd = 512 # Reduce the embedding size\n",
    "dropout = 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer\n",
    "learning_rate = 6e-4\n",
    "max_iters = 600000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = 600000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'gloo'\n",
    "\n",
    "# System\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "dtype = 'float32'  # MPS currently supports only float32\n",
    "compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Collect configuration keys\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "\n",
    "# Various initializations and derived attributes\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n",
    "    torch.mps.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    seed_offset = ddp_rank\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'mps' if 'mps' in device else 'cpu'\n",
    "ptdtype = torch.float32  # Currently, MPS supports only float32\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "Number of parameters: 22.42M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(6861, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=6861, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize iteration number and best validation loss\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# Model initialization\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=len(dataset.input_ids), dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GradScaler\n",
    "scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # Free up memory\n",
    "\n",
    "# Wrap model into DDP container if needed\n",
    "if ddp:\n",
    "    print(f\"Starting parallel process with rank {ddp_rank}\")\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# Function to estimate loss over splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n",
    "            for k in range(eval_iters):\n",
    "                batch = next(iter(dataloader))\n",
    "                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                pbar.update(1)\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 200/200 [00:26<00:00,  7.52it/s]\n",
      "Evaluating val: 100%|██████████| 200/200 [00:28<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.9129, val loss 6.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0: 100%|██████████| 40/40 [00:14<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 7.0153, time 69213.72ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1: 100%|██████████| 40/40 [00:13<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: loss 6.8796, time 13865.59ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2:  30%|███       | 12/40 [00:04<00:10,  2.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync \u001b[38;5;241m=\u001b[39m (micro_step \u001b[38;5;241m==\u001b[39m gradient_accumulation_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[0;32m---> 35\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     37\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_model = model.module if ddp else model\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "t0 = time.time()\n",
    "\n",
    "while iter_num <= max_iters:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            pbar.update(1)\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 511])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3315,  0.3442,  0.9170,  ...,  0.5294, -0.4510,  0.4243],\n",
       "          [ 0.2494,  0.5027,  0.8012,  ...,  0.6525, -0.3048,  0.3368],\n",
       "          [-0.0948,  0.6975,  0.8772,  ...,  0.4716, -0.2738,  0.6067],\n",
       "          ...,\n",
       "          [-0.3669, -0.8627,  0.8842,  ...,  0.8637,  0.1471, -0.0829],\n",
       "          [-0.1819, -0.4338,  0.6389,  ...,  0.6406,  0.0930, -0.2009],\n",
       "          [-0.7336, -0.1003,  0.4241,  ...,  0.5056,  0.4920, -0.4071]],\n",
       " \n",
       "         [[ 0.3878,  0.4814,  0.8031,  ..., -0.1710, -0.2604,  0.8452],\n",
       "          [ 0.3523,  0.3770,  0.6969,  ..., -0.1194, -0.2412,  0.8452],\n",
       "          [ 0.1664,  0.4927,  0.6952,  ..., -0.0504, -0.2668,  1.1063],\n",
       "          ...,\n",
       "          [ 0.0423,  0.2268,  0.5100,  ...,  0.1423,  0.2569, -0.1164],\n",
       "          [ 0.1000,  0.4063,  0.5384,  ...,  0.0562,  0.0893, -0.1400],\n",
       "          [ 0.0725,  0.4153,  0.3523,  ..., -0.0303,  0.2497, -0.0804]]],\n",
       "        device='mps:0', grad_fn=<LinearBackward0>),\n",
       " tensor(7.1700, device='mps:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x.to('mps'), y.to('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(6861, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=6861, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
