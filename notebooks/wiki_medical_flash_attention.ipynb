{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from typing import Optional, Union\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnielspace\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nielspace/Documents/Deep Learning/DL-research/NNResearch/notebooks/wandb/run-20240707_112714-gdj9dh68</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nielspace/Attention%20Benchmark/runs/gdj9dh68' target=\"_blank\">Flash Attention - mps</a></strong> to <a href='https://wandb.ai/nielspace/Attention%20Benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nielspace/Attention%20Benchmark' target=\"_blank\">https://wandb.ai/nielspace/Attention%20Benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nielspace/Attention%20Benchmark/runs/gdj9dh68' target=\"_blank\">https://wandb.ai/nielspace/Attention%20Benchmark/runs/gdj9dh68</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb_log = True  \n",
    "run_name = 'Flash Attention - mps'\n",
    "project_name = \"Attention Benchmark \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, articles, model=\"gpt2\", seq_length=512):\n",
    "        self.tokenizer = tiktoken.get_encoding(model)\n",
    "        self.vocab_size = self.tokenizer.n_vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.articles = articles.apply(self.preprocess_and_tokenize)\n",
    "        \n",
    "        self.input_ids, self.targets, self.attention_masks = self.create_sequences()\n",
    "\n",
    "    def preprocess_and_tokenize(self, text):\n",
    "        # Preprocess text\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Check for invalid token indices\n",
    "        assert all(token < self.vocab_size for token in tokens), \"Token index out of range\"\n",
    "        \n",
    "        # Pad and truncate tokens\n",
    "        if len(tokens) > self.seq_length:\n",
    "            tokens = tokens[:self.seq_length]\n",
    "        else:\n",
    "            tokens += [0] * (self.seq_length - len(tokens))\n",
    "        self.tokens = tokens\n",
    "        return tokens\n",
    "\n",
    "    def create_sequences(self):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        targets = []\n",
    "        \n",
    "        for tokens in self.articles:\n",
    "            input_ids.append(tokens[:-1])  # Exclude the last token for input\n",
    "            targets.append(tokens[1:])     # Exclude the first token for target\n",
    "            attention_masks.append([1 if token != 0 else 0 for token in tokens[:-1]])\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, targets, attention_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_seq = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        \n",
    "        sample = {'input_ids': input_seq, 'targets': target_seq, 'attention_mask': attention_mask}\n",
    "        return sample\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    targets = [item['targets'] for item in batch]\n",
    "\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'input_ids': input_ids_padded, 'targets': targets_padded, 'attention_mask': attention_masks_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "df = pd.read_parquet(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")\n",
    "articles = df.iloc[:, 1]\n",
    "\n",
    "dataset = TextDataset(articles, model=\"gpt2\", seq_length=512)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1845, 23253,   321,  ..., 14383,  2465,   287],\n",
      "        [  330,   398,  1533,  ...,   290, 24039,  1390]]) tensor([[23253,   321,   349,  ...,  2465,   287,  2276],\n",
      "        [  398,  1533,  3400,  ..., 24039,  1390, 32674]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    x, y, att = data['input_ids'], data['targets'], data['attention_mask']\n",
    "    print(x,y,att)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 511]), torch.Size([2, 511]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.dropout_p = config.dropout\n",
    "        self.causal = True  # assuming causal for GPT-like model\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        qkv = self.qkv(x).view(b, t, 3, self.n_head, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        q, k, v = [rearrange(x, 'b t h d -> (b h) t d') for x in (q, k, v)]\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device, dtype=torch.bool), diagonal=1)\n",
    "            attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = rearrange(attn_output, '(b h) t d -> b t (h d)', h=self.n_head)\n",
    "        attn_output = self.proj(attn_output)\n",
    "\n",
    "        # Save tensors for backward pass\n",
    "        self.saved_tensors = (q, k, v, attn_weights)\n",
    "        return attn_output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        q, k, v, attn_weights = self.saved_tensors\n",
    "\n",
    "        # Gradient of attention output\n",
    "        datt = torch.matmul(dout, v.transpose(-2, -1))\n",
    "        dv = torch.matmul(attn_weights.transpose(-2, -1), dout)\n",
    "\n",
    "        # Gradient of attention weights\n",
    "        datt_weights = dout @ v.transpose(-2, -1)\n",
    "\n",
    "        # Masked fill in the gradient if causal\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones(datt.size(-2), datt.size(-1), device=datt.device, dtype=torch.bool), diagonal=1)\n",
    "            datt = datt.masked_fill(mask, 0.0)\n",
    "\n",
    "        # Gradient of Q, K, V\n",
    "        dq = datt @ k.transpose(-2, -1)\n",
    "        dk = datt.transpose(-2, -1) @ q\n",
    "        dq = dq.view_as(q)\n",
    "        dk = dk.view_as(k)\n",
    "        dv = dv.view_as(v)\n",
    "\n",
    "        return dq, dk, dv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.attn = FlashAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Assuming GPTConfig is defined somewhere in the code\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None, \"Config must include vocab_size\"\n",
    "        assert config.block_size is not None, \"Config must include block_size\"\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'drop': nn.Dropout(config.dropout),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=1e-5),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.init_residuals()\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def init_residuals(self):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = logits[:, [-1], :]  # Use only the last token's logits\n",
    "            return logits, None\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}, \"Invalid model type\"\n",
    "        override_args = override_args or {}\n",
    "        assert all(k == 'dropout' for k in override_args), \"Only 'dropout' can be overridden\"\n",
    "\n",
    "        print(f\"Loading weights from pretrained model: {model_type}\")\n",
    "\n",
    "        config_args = cls.get_config_args(model_type)\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_pretrained_weights(model_type)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_args(model_type):\n",
    "        config_map = {\n",
    "            'gpt2': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'gpt2-medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'gpt2-large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'gpt2-xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
    "        }\n",
    "        config_args = config_map[model_type]\n",
    "        config_args.update({'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'dropout': 0.1})\n",
    "        return config_args\n",
    "\n",
    "    def load_pretrained_weights(self, model_type):\n",
    "        model_hf = transformers.GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd = self.state_dict()\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for k, v in sd_hf.items():\n",
    "            if k in sd:\n",
    "                if any(k.endswith(w) for w in transposed):\n",
    "                    sd[k].copy_(v.t())\n",
    "                else:\n",
    "                    sd[k].copy_(v)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        N = self.get_num_params()\n",
    "        L, H, Q, T = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.block_size\n",
    "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        flops_achieved = flops_per_iter * (1.0 / dt)\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dependencies\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import nullcontext\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per iteration will be: 40,960\n"
     ]
    }
   ],
   "source": [
    "# Default config values designed to train a GPT-2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# Data\n",
    "# data_path = 'data/openwebtext.txt'  # Path to your text file\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 2\n",
    "block_size = 512\n",
    "\n",
    "# Model\n",
    "n_layer = 6  # Reduce the number of layers\n",
    "n_head = 8   # Reduce the number of attention heads\n",
    "n_embd = 512 # Reduce the embedding size\n",
    "dropout = 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer\n",
    "learning_rate = 6e-4\n",
    "max_iters = 1000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 200\n",
    "lr_decay_iters = 1000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'gloo'\n",
    "\n",
    "# System\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "dtype = 'float32'  # MPS currently supports only float32\n",
    "compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Collect configuration keys\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "\n",
    "# Various initializations and derived attributes\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n",
    "    torch.mps.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    seed_offset = ddp_rank\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'mps' if 'mps' in device else 'cpu'\n",
    "ptdtype = torch.float32  # Currently, MPS supports only float32\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "Number of parameters: 19.18M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(512, 512)\n",
       "    (wpe): Embedding(512, 512)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): FlashAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        \n",
    "# Initialize iteration number and best validation loss\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# Model initialization\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GradScaler\n",
    "scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # Free up memory\n",
    "\n",
    "# Wrap model into DDP container if needed\n",
    "if ddp:\n",
    "    print(f\"Starting parallel process with rank {ddp_rank}\")\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# Function to estimate loss over splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n",
    "            for k in range(eval_iters):\n",
    "                batch = next(iter(dataloader))\n",
    "                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                pbar.update(1)\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 200/200 [00:16<00:00, 12.11it/s]\n",
      "Evaluating val: 100%|██████████| 200/200 [00:14<00:00, 13.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.0631, val loss 2.0631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0: 100%|██████████| 40/40 [00:08<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 2.0631, time 40256.62ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1: 100%|██████████| 40/40 [00:07<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: loss 2.0631, time 7408.92ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2: 100%|██████████| 40/40 [00:07<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2: loss 2.0370, time 7600.01ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3: 100%|██████████| 40/40 [00:07<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3: loss 1.9862, time 7310.86ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 4: 100%|██████████| 40/40 [00:07<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4: loss 1.9157, time 7519.17ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 5: 100%|██████████| 40/40 [00:07<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5: loss 1.8344, time 7514.98ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 6: 100%|██████████| 40/40 [00:07<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6: loss 1.7532, time 7375.65ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 7: 100%|██████████| 40/40 [00:07<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7: loss 1.6792, time 7466.66ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 8: 100%|██████████| 40/40 [00:07<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8: loss 1.6144, time 7420.53ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 9: 100%|██████████| 40/40 [00:07<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9: loss 1.5575, time 7545.67ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 10: 100%|██████████| 40/40 [00:07<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10: loss 1.5062, time 7863.78ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 11: 100%|██████████| 40/40 [00:07<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11: loss 1.4590, time 7680.80ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 12: 100%|██████████| 40/40 [00:07<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12: loss 1.4162, time 7572.87ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 13: 100%|██████████| 40/40 [00:07<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13: loss 1.3782, time 7892.51ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 14: 100%|██████████| 40/40 [00:07<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14: loss 1.3433, time 7577.11ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 15: 100%|██████████| 40/40 [00:07<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15: loss 1.3095, time 7428.24ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 16: 100%|██████████| 40/40 [00:07<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16: loss 1.2762, time 7443.84ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 17: 100%|██████████| 40/40 [00:07<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17: loss 1.2419, time 7499.61ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 18: 100%|██████████| 40/40 [00:07<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18: loss 1.2038, time 7477.60ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 19: 100%|██████████| 40/40 [00:07<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19: loss 1.1608, time 7506.00ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 20: 100%|██████████| 40/40 [00:07<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20: loss 1.1105, time 7619.94ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 21: 100%|██████████| 40/40 [00:07<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21: loss 1.0525, time 7612.05ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 22: 100%|██████████| 40/40 [00:07<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22: loss 0.9858, time 7787.98ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 23: 100%|██████████| 40/40 [00:07<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23: loss 0.9110, time 7855.57ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 24: 100%|██████████| 40/40 [00:07<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24: loss 0.8302, time 7884.18ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 25: 100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25: loss 0.7505, time 7919.98ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 26: 100%|██████████| 40/40 [00:07<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26: loss 0.6756, time 7973.80ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 27: 100%|██████████| 40/40 [00:07<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27: loss 0.6044, time 8047.79ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 28: 100%|██████████| 40/40 [00:07<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28: loss 0.5374, time 7942.95ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 29: 100%|██████████| 40/40 [00:07<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29: loss 0.4745, time 8038.33ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 30: 100%|██████████| 40/40 [00:07<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30: loss 0.4175, time 8028.50ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 31: 100%|██████████| 40/40 [00:07<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 31: loss 0.3619, time 8005.61ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 32: 100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 32: loss 0.3094, time 8108.67ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 33: 100%|██████████| 40/40 [00:07<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 33: loss 0.2615, time 8058.33ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 34: 100%|██████████| 40/40 [00:07<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34: loss 0.2189, time 8069.16ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 35: 100%|██████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35: loss 0.1823, time 8435.42ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 36: 100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 36: loss 0.1529, time 8104.90ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 37: 100%|██████████| 40/40 [00:08<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 37: loss 0.1270, time 8207.21ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 38: 100%|██████████| 40/40 [00:08<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38: loss 0.1056, time 8203.18ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 39: 100%|██████████| 40/40 [00:07<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 39: loss 0.0877, time 8092.11ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 40: 100%|██████████| 40/40 [00:08<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 40: loss 0.0721, time 8242.59ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 41: 100%|██████████| 40/40 [00:07<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 41: loss 0.0603, time 8140.46ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 42: 100%|██████████| 40/40 [00:08<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42: loss 0.0509, time 8211.24ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 43: 100%|██████████| 40/40 [00:07<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 43: loss 0.0427, time 8104.03ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 44: 100%|██████████| 40/40 [00:07<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 44: loss 0.0365, time 8162.46ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 45: 100%|██████████| 40/40 [00:08<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45: loss 0.0312, time 8259.21ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 46: 100%|██████████| 40/40 [00:08<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 46: loss 0.0266, time 8293.51ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 47: 100%|██████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 47: loss 0.0232, time 8506.63ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 48: 100%|██████████| 40/40 [00:08<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 48: loss 0.0200, time 8523.97ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 49: 100%|██████████| 40/40 [00:08<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49: loss 0.0175, time 8755.38ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 50: 100%|██████████| 40/40 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 50: loss 0.0156, time 8718.53ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 51: 100%|██████████| 40/40 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 51: loss 0.0138, time 8885.66ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 52: 100%|██████████| 40/40 [00:08<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 52: loss 0.0124, time 9072.02ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 53: 100%|██████████| 40/40 [00:08<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53: loss 0.0113, time 8939.51ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 54: 100%|██████████| 40/40 [00:08<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 54: loss 0.0103, time 9149.96ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 55: 100%|██████████| 40/40 [00:08<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 55: loss 0.0094, time 9023.60ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 56: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 56: loss 0.0087, time 9183.68ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 57: 100%|██████████| 40/40 [00:08<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57: loss 0.0081, time 9049.70ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 58: 100%|██████████| 40/40 [00:09<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 58: loss 0.0075, time 9438.60ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 59: 100%|██████████| 40/40 [00:08<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 59: loss 0.0070, time 8905.91ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 60: 100%|██████████| 40/40 [00:08<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 60: loss 0.0065, time 9076.98ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 61: 100%|██████████| 40/40 [00:08<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61: loss 0.0061, time 9048.17ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 62: 100%|██████████| 40/40 [00:08<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 62: loss 0.0058, time 8863.46ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 63: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 63: loss 0.0055, time 8970.57ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 64: 100%|██████████| 40/40 [00:08<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 64: loss 0.0052, time 8819.27ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 65: 100%|██████████| 40/40 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65: loss 0.0049, time 8867.86ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 66: 100%|██████████| 40/40 [00:08<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 66: loss 0.0047, time 8775.40ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 67: 100%|██████████| 40/40 [00:08<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 67: loss 0.0044, time 8896.07ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 68: 100%|██████████| 40/40 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 68: loss 0.0042, time 8775.35ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 69: 100%|██████████| 40/40 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69: loss 0.0040, time 8881.22ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 70: 100%|██████████| 40/40 [00:08<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 70: loss 0.0038, time 8857.56ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 71: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 71: loss 0.0037, time 9014.34ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 72: 100%|██████████| 40/40 [00:09<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 72: loss 0.0035, time 9378.52ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 73: 100%|██████████| 40/40 [00:08<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73: loss 0.0034, time 8902.64ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 74: 100%|██████████| 40/40 [00:08<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 74: loss 0.0033, time 9008.57ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 75: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 75: loss 0.0031, time 9003.13ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 76: 100%|██████████| 40/40 [00:08<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 76: loss 0.0030, time 9048.31ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 77: 100%|██████████| 40/40 [00:08<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 77: loss 0.0029, time 8952.70ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 78: 100%|██████████| 40/40 [00:09<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 78: loss 0.0028, time 9438.60ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 79: 100%|██████████| 40/40 [00:08<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 79: loss 0.0027, time 9103.43ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 80: 100%|██████████| 40/40 [00:09<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 80: loss 0.0026, time 9611.62ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 81: 100%|██████████| 40/40 [00:08<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 81: loss 0.0026, time 9180.91ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 82: 100%|██████████| 40/40 [00:09<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 82: loss 0.0025, time 9557.66ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 83: 100%|██████████| 40/40 [00:09<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 83: loss 0.0024, time 9329.30ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 84: 100%|██████████| 40/40 [00:09<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 84: loss 0.0023, time 9443.55ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 85: 100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 85: loss 0.0023, time 9631.82ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 86: 100%|██████████| 40/40 [00:09<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 86: loss 0.0022, time 9421.13ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 87: 100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 87: loss 0.0021, time 9647.76ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 88: 100%|██████████| 40/40 [00:10<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 88: loss 0.0021, time 10939.10ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 89: 100%|██████████| 40/40 [00:10<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 89: loss 0.0020, time 10314.47ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 90: 100%|██████████| 40/40 [00:09<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 90: loss 0.0020, time 9946.12ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 91: 100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 91: loss 0.0019, time 10509.44ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 92: 100%|██████████| 40/40 [00:09<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 92: loss 0.0019, time 10225.55ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 93: 100%|██████████| 40/40 [00:10<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 93: loss 0.0018, time 11206.19ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 94: 100%|██████████| 40/40 [00:12<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 94: loss 0.0018, time 12804.55ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 95: 100%|██████████| 40/40 [00:12<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 95: loss 0.0017, time 12545.42ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 96: 100%|██████████| 40/40 [00:11<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 96: loss 0.0017, time 11539.59ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 97: 100%|██████████| 40/40 [00:11<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 97: loss 0.0017, time 11446.81ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 98: 100%|██████████| 40/40 [00:11<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 98: loss 0.0016, time 11319.44ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 99: 100%|██████████| 40/40 [00:09<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 99: loss 0.0016, time 10171.25ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 100: 100%|██████████| 40/40 [00:09<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: loss 0.0015, time 10171.70ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 101:   8%|▊         | 3/40 [00:00<00:09,  4.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync \u001b[38;5;241m=\u001b[39m (micro_step \u001b[38;5;241m==\u001b[39m gradient_accumulation_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[0;32m---> 50\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     52\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=project_name, name= run_name, config=config)\n",
    "\n",
    "raw_model = model.module if ddp else model\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "t0 = time.time()\n",
    "\n",
    "while iter_num <= max_iters:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train_loss\": losses['train'],\n",
    "                \"val_loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "            })\n",
    "\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n",
    "        total_train_loss = 0.0  # Initialize total training loss\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            pbar.update(1)\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train_loss\": total_train_loss,\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "            })\n",
    "\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
