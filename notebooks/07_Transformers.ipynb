{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29c7a",
   "metadata": {},
   "source": [
    "# Attention workflow\n",
    "1. Input \n",
    "2. Word embeddings\n",
    "3. Positional embeddings\n",
    "4. Concat(Word, Positional)\n",
    "5. Normalization (optional)\n",
    "6. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3336fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1,115,394\n",
      "All the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "Train has 1,003,854 tokens\n",
      "Val has 111,540 tokens\n",
      "Preparing train batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Download:\n",
    "    def __init__(self, path, data_url, create_split: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the Download class.\n",
    "        \n",
    "        :param path: The path where the data will be saved.\n",
    "        :param data_url: The URL to download the data from.\n",
    "        :param create_split: Whether to create train/validation split.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.data_url = data_url\n",
    "        self.create_split = create_split\n",
    "\n",
    "    def fetch(self):\n",
    "        \"\"\"\n",
    "        Fetch the data from the specified URL and save it to the specified path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create the 'data' directory relative to the current directory\n",
    "            data_dir = os.path.join(os.getcwd(), 'data')\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "            # Construct the full file path\n",
    "            input_file_path = os.path.join(data_dir, self.path)\n",
    "\n",
    "            # Download the file if it doesn't exist\n",
    "            if not os.path.exists(input_file_path):\n",
    "                response = requests.get(self.data_url)\n",
    "                response.raise_for_status()  # Raise an error for bad status codes\n",
    "                with open(input_file_path, 'w') as f:\n",
    "                    f.write(response.text)\n",
    "\n",
    "            # Read and print the length of the dataset\n",
    "            with open(input_file_path, 'r') as f:\n",
    "                self.data = f.read()\n",
    "            print(f\"Length of dataset in characters: {len(self.data):,}\")\n",
    "            self.preprocessing()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading the file: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data to create character mappings and encoding functions.\n",
    "        \"\"\"\n",
    "        # Get all the unique characters that occur in this text\n",
    "        chars = sorted(list(set(self.data)))\n",
    "        vocab_size = len(chars)\n",
    "        print(\"All the unique characters:\", ''.join(chars))\n",
    "        print(f\"Vocab size: {vocab_size:,}\")\n",
    "\n",
    "        # Create a mapping from characters to integers\n",
    "        stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "    def split(self, size=0.9):\n",
    "        \"\"\"\n",
    "        Create train and validation splits from the data.\n",
    "        \n",
    "        :param size: The proportion of data to use for training.\n",
    "        :return: Encoded training and validation data.\n",
    "        \"\"\"\n",
    "        n = len(self.data)\n",
    "        train_data = self.data[:int(n * size)]\n",
    "        val_data = self.data[int(n * size):]  \n",
    "\n",
    "        # Encoding\n",
    "        train_ids = self.encode(train_data)\n",
    "        val_ids = self.encode(val_data)\n",
    "        print(f\"Train has {len(train_ids):,} tokens\")\n",
    "        print(f\"Val has {len(val_ids):,} tokens\")\n",
    "\n",
    "        return train_ids, val_ids\n",
    "    \n",
    "    def get_batch(self, train_ids=None, val_ids=None, split: str = 'train', context_len: int = 1000, batch_size: int = 8, device_type: str = 'mps', device: str = 'mps'):\n",
    "        \"\"\"\n",
    "        Generate batches of data for training or validation.\n",
    "        \n",
    "        :param train_ids: Encoded training data.\n",
    "        :param val_ids: Encoded validation data.\n",
    "        :param split: Whether to use 'train' or 'val' data.\n",
    "        :param context_len: Length of the context for each sample.\n",
    "        :param batch_size: Number of samples per batch.\n",
    "        :param device_type: Type of device ('cuda' or other).\n",
    "        :param device: Specific device identifier.\n",
    "        :return: Batch of input and target data.\n",
    "        \"\"\"\n",
    "        data = train_ids if split == 'train' else val_ids\n",
    "        print(f\"Preparing {split} batch\")\n",
    "\n",
    "        ix = torch.randint(len(data) - context_len, (batch_size,))\n",
    "        x = torch.stack([torch.tensor(data[i:i + context_len]) for i in ix])\n",
    "        y = torch.stack([torch.tensor(data[i + 1:i + 1 + context_len]) for i in ix])\n",
    "        \n",
    "        if device_type == 'cuda':\n",
    "            # Pin arrays x, y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'tiny_shakespeare.txt'\n",
    "URL = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "downloader = Download(input_file, URL)\n",
    "downloader.fetch()\n",
    "train_ids, val_ids = downloader.split()\n",
    "x, y = downloader.get_batch(train_ids=train_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886d66ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45, 46, 39,  ..., 50,  8,  0],\n",
       "        [43, 39, 60,  ..., 57,  1, 59],\n",
       "        [47, 60, 43,  ..., 52, 53,  1],\n",
       "        ...,\n",
       "        [58, 39, 58,  ..., 57,  1, 41],\n",
       "        [63,  1, 39,  ..., 50, 39, 41],\n",
       "        [58, 46, 43,  ..., 43,  8,  0]], device='mps:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a767043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size=50276, n_embd=768, block_size=1024, device='mps'):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_embd).to(device)\n",
    "        self.wpe = nn.Embedding(block_size, n_embd).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=x.device)\n",
    "        T = self.wte(x)\n",
    "        P = self.wpe(pos)[None, :, :].expand(b, t, -1)\n",
    "        o = T + P\n",
    "        return o\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd=768, n_head=12, dropout=0.1, block_size=1024, device='mps'):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_embd, 3 * n_embd).to(device)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd).to(device)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50276, n_embd=768, n_head=12, dropout=0.1, block_size=1024, device='mps'):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, n_embd, block_size, device)\n",
    "        self.attention = CausalSelfAttention(n_embd, n_head, dropout, block_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.attention(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "# E = Embeddings(vocab_size=50276, n_embd=768, block_size=1024, device='mps')\n",
    "# A = CausalSelfAttention(n_embd=768, n_head=12, dropout=0.1, block_size=1024, device='mps')\n",
    "# A(E(x))\n",
    "\n",
    "model = TransformerModel()\n",
    "logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdd1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000, 768])\n"
     ]
    }
   ],
   "source": [
    "# class Embeddings(nn.Module):\n",
    "#     def __init__(self, vocab_size=50276, n_embd=768, block_size=1024, device='mps'):\n",
    "#         super().__init__()\n",
    "#         self.wte = nn.Embedding(vocab_size, n_embd).to(device)\n",
    "#         self.wpe = nn.Embedding(block_size, n_embd).to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         b, t = x.size()\n",
    "#         pos = torch.arange(0, t, dtype=torch.long, device=x.device)\n",
    "#         T = self.wte(x)\n",
    "#         P = self.wpe(pos)[None, :, :].expand(b, t, -1)\n",
    "#         return T + P\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, n_embd=768, n_head=12, dropout=0.1, block_size=1024, device='mps'):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_embd, 3 * n_embd).to(device)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd).to(device)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Flash Attention computation\n",
    "        q = q / math.sqrt(C // self.n_head)  # Scaling\n",
    "        attn_weights = torch.einsum('bhqd, bhkd -> bhqk', q, k)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        y = torch.einsum('bhqk, bhvd -> bhqd', attn_weights, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50276, n_embd=768, n_head=12, dropout=0.1, block_size=1024, device='mps'):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, n_embd, block_size, device)\n",
    "        self.attention = FlashAttention(n_embd, n_head, dropout, block_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.attention(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = TransformerModel()\n",
    "output = model(x)\n",
    "\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72cdb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# class Embeddings(nn.Module):\n",
    "#     def __init__(self, vocab_size=50276, n_embd=768, block_size=1024, device='mps'):\n",
    "#         super().__init__()\n",
    "#         self.wte = nn.Embedding(vocab_size, n_embd).to(device)\n",
    "#         self.wpe = nn.Embedding(block_size, n_embd).to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         b, t = x.size()\n",
    "#         pos = torch.arange(0, t, dtype=torch.long, device=x.device)\n",
    "#         T = self.wte(x)\n",
    "#         P = self.wpe(pos)[None, :, :].expand(b, t, -1)\n",
    "#         return T + P\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, n_embd=768, n_head=12, dropout=0.1, block_size=1024, sparsity_pattern=None, device='mps'):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_pattern = sparsity_pattern\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_embd, 3 * n_embd).to(device)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd).to(device)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Sparse Attention computation\n",
    "        attn_weights = torch.einsum('bhqd, bhkd -> bhqk', q, k) / math.sqrt(C // self.n_head)\n",
    "        \n",
    "        if self.sparsity_pattern is not None:\n",
    "            attn_weights = attn_weights.masked_fill(self.sparsity_pattern == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        \n",
    "        y = torch.einsum('bhqk, bhvd -> bhqd', attn_weights, v)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50276, n_embd=768, n_head=12, dropout=0.1, block_size=1024, sparsity_pattern=None, device='mps'):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, n_embd, block_size, device)\n",
    "        self.attention = SparseAttention(n_embd, n_head, dropout, block_size, sparsity_pattern, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.attention(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a sparsity pattern (example: attend to only a subset of tokens)\n",
    "sparsity_pattern = torch.ones(1, 12, 1024, 1024, device=device)  # Modify this pattern as needed\n",
    "# For example, you can create a block sparsity pattern\n",
    "for i in range(0, 1024, 32):\n",
    "    for j in range(0, 1024, 32):\n",
    "        if (i // 32 + j // 32) % 2 == 0:\n",
    "            sparsity_pattern[:, :, i:i+32, j:j+32] = 0\n",
    "\n",
    "x = torch.randint(1, 50276, (1, 1024), device=device)\n",
    "model = TransformerModel(sparsity_pattern=sparsity_pattern, device=device).to(device)\n",
    "output = model(x)\n",
    "\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb57664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, n_embd=768, n_head=12, dropout=0.1, window_size=128, device='mps'):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_embd, 3 * n_embd).to(device)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd).to(device)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        output = []\n",
    "        for i in range(T):\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(T, i + self.window_size)\n",
    "            q_i = q[:, :, i, :]\n",
    "            k_i = k[:, :, start:end, :]\n",
    "            v_i = v[:, :, start:end, :]\n",
    "\n",
    "            attn_weights = torch.einsum('bhd,bhjd->bhj', q_i, k_i) / math.sqrt(C // self.n_head)\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "            context = torch.einsum('bhj,bhjd->bhd', attn_weights, v_i)\n",
    "            output.append(context)\n",
    "\n",
    "        output = torch.stack(output, dim=2).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output = self.resid_dropout(self.c_proj(output))\n",
    "        return output\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size=50276, n_embd=768, n_head=12, dropout=0.1, block_size=1024, window_size=128, device='mps'):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(vocab_size, n_embd, block_size, device)\n",
    "        self.attention = LocalAttention(n_embd, n_head, dropout, window_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.attention(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# x = torch.randint(1, 50276, (1, 1024), device=device)\n",
    "model = TransformerModel()\n",
    "output = model(x)\n",
    "\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b046905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
