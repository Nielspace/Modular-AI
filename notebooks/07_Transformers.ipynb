{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17aaedce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (wte): Embedding(20, 50)\n",
       "  (wpe): Embedding(2, 50)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleDict(dict(\n",
    "    wte = nn.Embedding(20, 50),\n",
    "    wpe = nn.Embedding(2, 50), \n",
    "    drop = nn.Dropout(0.2),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29c7a",
   "metadata": {},
   "source": [
    "# Attention workflow\n",
    "1. Input \n",
    "2. Word embeddings\n",
    "3. Positional embeddings\n",
    "4. Concat(Word, Positional)\n",
    "5. Normalization (optional)\n",
    "6. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e2ec2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50276])\n",
      "torch.Size([32768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example dimensions\n",
    "B = 32  # Batch size\n",
    "T = 1024  # Sequence length or context window\n",
    "C = 128 # Embedding dimension\n",
    "\n",
    "# Generate tokenized input\n",
    "x = torch.randint(0, 50276, (50276,))\n",
    "print(x.shape)\n",
    "# Ensure the length of x is divisible by T\n",
    "num_tokens = B * T\n",
    "x = x[:num_tokens]  # Truncate or pad x as necessary\n",
    "\n",
    "print(x.shape)\n",
    "# Reshape to (B, T)\n",
    "x = x.view(B, T)\n",
    "\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa1323c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(1, 50276, (1, 1, 50276))\n",
    "#token embeddings\n",
    "def txt_emb(vocab_size=50276, n_emb=768):\n",
    "    wte = nn.Embedding(vocab_size, n_emb)\n",
    "    return wte\n",
    "\n",
    "#positional embeddings\n",
    "def pos_emb(block_size=1024, n_embd=768):\n",
    "    wpe = nn.Embedding(block_size, n_embd)\n",
    "    return wpe\n",
    "\n",
    "T = txt_emb()(x)\n",
    "b, t = x.size()\n",
    "pos = torch.arange(0, t)\n",
    "P = pos_emb()(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4234dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1024, 768]), torch.Size([1024, 768]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.size(), P.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d97b5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_x = (T + P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "591330f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd: int=768, \n",
    "                 n_head: int=12, \n",
    "                 dropout: float=0.1, \n",
    "                 block_size: int=1024):\n",
    "        super().__init__()\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.block_size = block_size\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        self.lin = nn.Linear(self.n_embd, 3 * self.n_embd, bias=True)\n",
    "\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=True)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        q, k, v  = self.lin(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        \n",
    "        # output\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "attn = CausalSelfAttention()\n",
    "attn = attn(concat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "df846216",
   "metadata": {},
   "outputs": [],
   "source": [
    "del attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a767043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
