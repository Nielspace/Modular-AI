{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17aaedce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (wte): Embedding(20, 50)\n",
       "  (wpe): Embedding(2, 50)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleDict(dict(\n",
    "    wte = nn.Embedding(20, 50),\n",
    "    wpe = nn.Embedding(2, 50), \n",
    "    drop = nn.Dropout(0.2),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29c7a",
   "metadata": {},
   "source": [
    "# Attention workflow\n",
    "1. Input \n",
    "2. Word embeddings\n",
    "3. Positional embeddings\n",
    "4. Concat(Word, Positional)\n",
    "5. Normalization (optional)\n",
    "6. Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2ec2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50276])\n",
      "torch.Size([1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example dimensions\n",
    "B = 32  # Batch size\n",
    "T = 1024  # Sequence length or context window\n",
    "C = 128 # Embedding dimension\n",
    "\n",
    "# Generate tokenized input\n",
    "x = torch.randint(0, 50276, (50276,))\n",
    "print(x.shape)\n",
    "# Ensure the length of x is divisible by T\n",
    "num_tokens = B * T\n",
    "x = x[:num_tokens]  # Truncate or pad x as necessary\n",
    "\n",
    "print(x.shape)\n",
    "# Reshape to (B, T)\n",
    "x = x.view(B, T)\n",
    "\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "77e712dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dimensions\n",
    "B = 32  # Batch size\n",
    "T = 50  # Desired sequence length\n",
    "C = 128 # Embedding dimension\n",
    "\n",
    "# Generate a long tokenized input sequence\n",
    "x = torch.randint(0, 50276, (50276,))  # Example long sequence\n",
    "\n",
    "# Chunk the sequence into non-overlapping segments of length T\n",
    "chunks = [x[i:i + T] for i in range(0, len(x), T)]\n",
    "\n",
    "# Ensure each chunk is of length T (pad if necessary)\n",
    "def pad_sequence(seq, length):\n",
    "    if len(seq) < length:\n",
    "        return torch.cat([seq, torch.zeros(length - len(seq), dtype=torch.long)])\n",
    "    return seq\n",
    "\n",
    "chunks = [pad_sequence(chunk, T) for chunk in chunks]\n",
    "\n",
    "# Create a batch from the chunks\n",
    "batches = [chunks[i:i + B] for i in range(0, len(chunks), B)]\n",
    "len(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fa1323c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wpe\n\u001b[1;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m txt_emb()(x)\n\u001b[0;32m---> 12\u001b[0m b, t \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     13\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, t)\n\u001b[1;32m     14\u001b[0m P \u001b[38;5;241m=\u001b[39m pos_emb()(pos)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#token embeddings\n",
    "def txt_emb(vocab_size=50276, n_emb=768):\n",
    "    wte = nn.Embedding(vocab_size, n_emb)\n",
    "    return wte\n",
    "\n",
    "#positional embeddings\n",
    "def pos_emb(block_size=1024, n_embd=768):\n",
    "    wpe = nn.Embedding(block_size, n_embd)\n",
    "    return wpe\n",
    "\n",
    "T = txt_emb()(x)\n",
    "b, t = x.size()\n",
    "pos = torch.arange(0, t)\n",
    "P = pos_emb()(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4234dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/2406.17763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b5ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
