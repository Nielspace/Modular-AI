{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'tiny_shakespeare.txt'\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_file_path = os.path.join(data_dir, input_file)\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, model=\"gpt2\", seq_length=400):\n",
    "        tokenizer = tiktoken.get_encoding(model)\n",
    "        self.tokens = tokenizer.encode(data)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.x, self.y = self.create_sequences()\n",
    "\n",
    "    def create_sequences(self):\n",
    "        x, y = [], []\n",
    "        for i in range(0, len(self.tokens) - self.seq_length, self.seq_length):\n",
    "            x.append(self.tokens[i:i+self.seq_length])\n",
    "            y.append(self.tokens[i+1:i+1+self.seq_length])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_seq = torch.tensor(self.x[idx], dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        sample = {'input': input_seq, 'target': target_seq}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(batch):\n",
    "    input_seqs = [item['input'] for item in batch]\n",
    "    target_seqs = [item['target'] for item in batch]\n",
    "\n",
    "    input_padded = torch.nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_padded = torch.nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'input': input_padded, 'target': target_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 8421,\n",
       " 356,\n",
       " 5120,\n",
       " 597,\n",
       " 2252,\n",
       " 11,\n",
       " 3285,\n",
       " 502,\n",
       " 2740,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 5248,\n",
       " 461,\n",
       " 11,\n",
       " 2740,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 1639,\n",
       " 389,\n",
       " 477,\n",
       " 12939,\n",
       " 2138,\n",
       " 284,\n",
       " 4656,\n",
       " 621,\n",
       " 284,\n",
       " 1145,\n",
       " 680,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 4965,\n",
       " 5634,\n",
       " 13,\n",
       " 12939,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 5962,\n",
       " 11,\n",
       " 345,\n",
       " 760,\n",
       " 327,\n",
       " 1872,\n",
       " 385,\n",
       " 1526,\n",
       " 28599,\n",
       " 318,\n",
       " 4039,\n",
       " 4472,\n",
       " 284,\n",
       " 262,\n",
       " 661,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 1135,\n",
       " 760,\n",
       " 470,\n",
       " 11,\n",
       " 356,\n",
       " 760,\n",
       " 470,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 5756,\n",
       " 514,\n",
       " 1494,\n",
       " 683,\n",
       " 11,\n",
       " 290,\n",
       " 356,\n",
       " 1183,\n",
       " 423,\n",
       " 11676,\n",
       " 379,\n",
       " 674,\n",
       " 898,\n",
       " 2756,\n",
       " 13,\n",
       " 198,\n",
       " 3792,\n",
       " 470,\n",
       " 257,\n",
       " 15593,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 2949,\n",
       " 517,\n",
       " 3375,\n",
       " 319,\n",
       " 470,\n",
       " 26,\n",
       " 1309,\n",
       " 340,\n",
       " 307,\n",
       " 1760,\n",
       " 25,\n",
       " 1497,\n",
       " 11,\n",
       " 1497,\n",
       " 0,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 3198,\n",
       " 1573,\n",
       " 11,\n",
       " 922,\n",
       " 4290,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 1135,\n",
       " 389,\n",
       " 17830,\n",
       " 3595,\n",
       " 4290,\n",
       " 11,\n",
       " 262,\n",
       " 1458,\n",
       " 1173,\n",
       " 1547,\n",
       " 922,\n",
       " 13,\n",
       " 198,\n",
       " 2061,\n",
       " 4934,\n",
       " 969,\n",
       " 5036,\n",
       " 896,\n",
       " 319,\n",
       " 561,\n",
       " 26958,\n",
       " 514,\n",
       " 25,\n",
       " 611,\n",
       " 484,\n",
       " 198,\n",
       " 19188,\n",
       " 7800,\n",
       " 514,\n",
       " 475,\n",
       " 262,\n",
       " 48713,\n",
       " 414,\n",
       " 11,\n",
       " 981,\n",
       " 340,\n",
       " 547,\n",
       " 198,\n",
       " 1929,\n",
       " 4316,\n",
       " 462,\n",
       " 11,\n",
       " 356,\n",
       " 1244,\n",
       " 4724,\n",
       " 484,\n",
       " 22598,\n",
       " 514,\n",
       " 31533,\n",
       " 306,\n",
       " 26,\n",
       " 198,\n",
       " 4360,\n",
       " 484,\n",
       " 892,\n",
       " 356,\n",
       " 389,\n",
       " 1165,\n",
       " 13674,\n",
       " 25,\n",
       " 262,\n",
       " 10904,\n",
       " 1108,\n",
       " 326,\n",
       " 198,\n",
       " 2001,\n",
       " 42267,\n",
       " 514,\n",
       " 11,\n",
       " 262,\n",
       " 2134,\n",
       " 286,\n",
       " 674,\n",
       " 24672,\n",
       " 11,\n",
       " 318,\n",
       " 355,\n",
       " 281,\n",
       " 198,\n",
       " 24807,\n",
       " 284,\n",
       " 1948,\n",
       " 786,\n",
       " 511,\n",
       " 20038,\n",
       " 26,\n",
       " 674,\n",
       " 198,\n",
       " 82,\n",
       " 13712,\n",
       " 590,\n",
       " 318,\n",
       " 257,\n",
       " 4461,\n",
       " 284,\n",
       " 606,\n",
       " 3914,\n",
       " 514,\n",
       " 15827,\n",
       " 428,\n",
       " 351,\n",
       " 198,\n",
       " 454,\n",
       " 279,\n",
       " 7938,\n",
       " 11,\n",
       " 304,\n",
       " 260,\n",
       " 356,\n",
       " 1716,\n",
       " 374,\n",
       " 1124,\n",
       " 25,\n",
       " 329,\n",
       " 262,\n",
       " 11858,\n",
       " 760,\n",
       " 314,\n",
       " 198,\n",
       " 47350,\n",
       " 428,\n",
       " 287,\n",
       " 16460,\n",
       " 329,\n",
       " 8509,\n",
       " 11,\n",
       " 407,\n",
       " 287,\n",
       " 24613,\n",
       " 329,\n",
       " 15827,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 17353,\n",
       " 345,\n",
       " 5120,\n",
       " 2592,\n",
       " 1028,\n",
       " 327,\n",
       " 1872,\n",
       " 385,\n",
       " 1526,\n",
       " 28599,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 39276,\n",
       " 683,\n",
       " 717,\n",
       " 25,\n",
       " 339,\n",
       " 338,\n",
       " 257,\n",
       " 845,\n",
       " 3290,\n",
       " 284,\n",
       " 262,\n",
       " 2219,\n",
       " 6017,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 19626,\n",
       " 345,\n",
       " 644,\n",
       " 2594,\n",
       " 339,\n",
       " 468,\n",
       " 1760,\n",
       " 329,\n",
       " 465,\n",
       " 1499,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 16371,\n",
       " 880,\n",
       " 26,\n",
       " 290,\n",
       " 714,\n",
       " 307,\n",
       " 2695,\n",
       " 284,\n",
       " 1577,\n",
       " 683,\n",
       " 922,\n",
       " 198,\n",
       " 13116,\n",
       " 6285,\n",
       " 11,\n",
       " 475,\n",
       " 326,\n",
       " 339,\n",
       " 13831,\n",
       " 2241,\n",
       " 351,\n",
       " 852,\n",
       " 6613,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 45,\n",
       " 323,\n",
       " 11,\n",
       " 475,\n",
       " 2740,\n",
       " 407,\n",
       " 17412,\n",
       " 306,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 40,\n",
       " 910,\n",
       " 12722,\n",
       " 345,\n",
       " 11,\n",
       " 644,\n",
       " 339,\n",
       " 22027,\n",
       " 1760,\n",
       " 20524,\n",
       " 11,\n",
       " 339,\n",
       " 750,\n",
       " 198,\n",
       " 270,\n",
       " 284,\n",
       " 326,\n",
       " 886,\n",
       " 25,\n",
       " 996,\n",
       " 2705,\n",
       " 12,\n",
       " 5936,\n",
       " 979,\n",
       " 5864,\n",
       " 1450,\n",
       " 460,\n",
       " 307,\n",
       " 198,\n",
       " 11299,\n",
       " 284,\n",
       " 910,\n",
       " 340,\n",
       " 373,\n",
       " 329,\n",
       " 465,\n",
       " 1499,\n",
       " 339,\n",
       " 750,\n",
       " 340,\n",
       " 284,\n",
       " 198,\n",
       " 29688,\n",
       " 465,\n",
       " 2802,\n",
       " 290,\n",
       " 284,\n",
       " 307,\n",
       " 11476,\n",
       " 6613,\n",
       " 26,\n",
       " 543,\n",
       " 339,\n",
       " 198,\n",
       " 271,\n",
       " 11,\n",
       " 772,\n",
       " 10597,\n",
       " 262,\n",
       " 20334,\n",
       " 286,\n",
       " 465,\n",
       " 14675,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 2061,\n",
       " 339,\n",
       " 2314,\n",
       " 1037,\n",
       " 287,\n",
       " 465,\n",
       " 3450,\n",
       " 11,\n",
       " 345,\n",
       " 1848,\n",
       " 257,\n",
       " 198,\n",
       " 28281,\n",
       " 287,\n",
       " 683,\n",
       " 13,\n",
       " 921,\n",
       " 1276,\n",
       " 287,\n",
       " 645,\n",
       " 835,\n",
       " 910,\n",
       " 339,\n",
       " 318,\n",
       " 25746,\n",
       " 83,\n",
       " 516,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 1532,\n",
       " 314,\n",
       " 1276,\n",
       " 407,\n",
       " 11,\n",
       " 314,\n",
       " 761,\n",
       " 407,\n",
       " 307,\n",
       " 39497,\n",
       " 286,\n",
       " 14227,\n",
       " 26,\n",
       " 198,\n",
       " 258,\n",
       " 22027,\n",
       " 31025,\n",
       " 11,\n",
       " 351,\n",
       " 18201,\n",
       " 11,\n",
       " 284,\n",
       " 15867,\n",
       " 287,\n",
       " 29693,\n",
       " 13,\n",
       " 198,\n",
       " 2061,\n",
       " 34757,\n",
       " 389,\n",
       " 777,\n",
       " 30,\n",
       " 383,\n",
       " 584,\n",
       " 1735,\n",
       " 267,\n",
       " 6,\n",
       " 262,\n",
       " 1748,\n",
       " 198,\n",
       " 271,\n",
       " 17450,\n",
       " 25,\n",
       " 1521,\n",
       " 2652,\n",
       " 356,\n",
       " 778,\n",
       " 803,\n",
       " 994,\n",
       " 30,\n",
       " 284,\n",
       " 262,\n",
       " 13241,\n",
       " 0,\n",
       " 198,\n",
       " 198,\n",
       " 3237,\n",
       " 25,\n",
       " 198,\n",
       " 16773,\n",
       " 11,\n",
       " 1282,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 18380,\n",
       " 0,\n",
       " 508,\n",
       " 2058,\n",
       " 994,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 12211,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 54,\n",
       " 18906,\n",
       " 6065,\n",
       " 268,\n",
       " 3754,\n",
       " 2449,\n",
       " 14602,\n",
       " 64,\n",
       " 26,\n",
       " 530,\n",
       " 326,\n",
       " 22027,\n",
       " 1464,\n",
       " 6151,\n",
       " 198,\n",
       " 1169,\n",
       " 661,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 1544,\n",
       " 338,\n",
       " 530,\n",
       " 5508,\n",
       " 1576,\n",
       " 25,\n",
       " 561,\n",
       " 477,\n",
       " 262,\n",
       " 1334,\n",
       " 547,\n",
       " 523,\n",
       " 0,\n",
       " 198,\n",
       " 198,\n",
       " 49275,\n",
       " 1677,\n",
       " 40,\n",
       " 2937,\n",
       " 25,\n",
       " 198,\n",
       " 2061,\n",
       " 670,\n",
       " 338,\n",
       " 11,\n",
       " 616,\n",
       " 1499,\n",
       " 3653,\n",
       " 11,\n",
       " 287,\n",
       " 1021,\n",
       " 30,\n",
       " 810,\n",
       " 467,\n",
       " 345,\n",
       " 198,\n",
       " 3152,\n",
       " 19553,\n",
       " 290,\n",
       " 9784,\n",
       " 30,\n",
       " 383,\n",
       " 2300,\n",
       " 30,\n",
       " 2740,\n",
       " 11,\n",
       " 314,\n",
       " 12472,\n",
       " 345,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 5122,\n",
       " 1597,\n",
       " 318,\n",
       " 407,\n",
       " 6439,\n",
       " 284,\n",
       " 262,\n",
       " 34548,\n",
       " 26,\n",
       " 484,\n",
       " 423,\n",
       " 198,\n",
       " 18108,\n",
       " 16882,\n",
       " 1359,\n",
       " 428,\n",
       " 46327,\n",
       " 644,\n",
       " 356,\n",
       " 14765,\n",
       " 284,\n",
       " 466,\n",
       " 11,\n",
       " 198,\n",
       " 4758,\n",
       " 783,\n",
       " 356,\n",
       " 1183,\n",
       " 905,\n",
       " 705,\n",
       " 368,\n",
       " 287,\n",
       " 23777,\n",
       " 13,\n",
       " 1119,\n",
       " 910,\n",
       " 3595,\n",
       " 198,\n",
       " 6063,\n",
       " 669,\n",
       " 423,\n",
       " 1913,\n",
       " 45576,\n",
       " 25,\n",
       " 484,\n",
       " 2236,\n",
       " 760,\n",
       " 356,\n",
       " 198,\n",
       " 14150,\n",
       " 1913,\n",
       " 5101,\n",
       " 1165,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 49275,\n",
       " 1677,\n",
       " 40,\n",
       " 2937,\n",
       " 25,\n",
       " 198,\n",
       " 5195,\n",
       " 11,\n",
       " 18159,\n",
       " 11,\n",
       " 616,\n",
       " 922,\n",
       " 2460,\n",
       " 11,\n",
       " 6164,\n",
       " 5508,\n",
       " 23788,\n",
       " 11,\n",
       " 198,\n",
       " 8743,\n",
       " 345,\n",
       " 23981,\n",
       " 27012,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 1135,\n",
       " 2314,\n",
       " 11,\n",
       " 15967,\n",
       " 11,\n",
       " 356,\n",
       " 389,\n",
       " 45171,\n",
       " 1541,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 49275,\n",
       " 1677,\n",
       " 40,\n",
       " 2937,\n",
       " 25,\n",
       " 198,\n",
       " 40,\n",
       " 1560,\n",
       " 345,\n",
       " 11,\n",
       " 2460,\n",
       " 11,\n",
       " 749,\n",
       " 21803,\n",
       " 1337,\n",
       " 198,\n",
       " 11980,\n",
       " 262,\n",
       " 1458,\n",
       " 1173,\n",
       " 1547,\n",
       " 286,\n",
       " 345,\n",
       " 13,\n",
       " 1114,\n",
       " 534,\n",
       " 3382,\n",
       " 11,\n",
       " 198,\n",
       " 7120,\n",
       " 7195,\n",
       " 287,\n",
       " 428,\n",
       " 390,\n",
       " 11999,\n",
       " 11,\n",
       " 345,\n",
       " 743,\n",
       " 355,\n",
       " 880,\n",
       " 198,\n",
       " 31584,\n",
       " 379,\n",
       " 262,\n",
       " 9538,\n",
       " 351,\n",
       " 534,\n",
       " 336,\n",
       " 3080,\n",
       " 355,\n",
       " 10303,\n",
       " 606,\n",
       " 198,\n",
       " 39276,\n",
       " 262,\n",
       " 7993,\n",
       " 1181,\n",
       " 11,\n",
       " 3025,\n",
       " 1781,\n",
       " 481,\n",
       " 319,\n",
       " 198,\n",
       " 464,\n",
       " 835,\n",
       " 340,\n",
       " 2753,\n",
       " 11,\n",
       " 25407,\n",
       " 3478,\n",
       " 7319,\n",
       " 1090,\n",
       " 1443,\n",
       " 198,\n",
       " 5189,\n",
       " 517,\n",
       " 1913,\n",
       " 2792,\n",
       " 355,\n",
       " 4625,\n",
       " 621,\n",
       " 460,\n",
       " 1683,\n",
       " 198,\n",
       " 4677,\n",
       " 451,\n",
       " 287,\n",
       " 534,\n",
       " 26795,\n",
       " 3681,\n",
       " 13,\n",
       " 1114,\n",
       " 262,\n",
       " 390,\n",
       " 11999,\n",
       " 11,\n",
       " 198,\n",
       " 464,\n",
       " 11858,\n",
       " 11,\n",
       " 407,\n",
       " 262,\n",
       " 1458,\n",
       " 1173,\n",
       " 1547,\n",
       " 11,\n",
       " 787,\n",
       " 340,\n",
       " 11,\n",
       " 290,\n",
       " 198,\n",
       " 7120,\n",
       " 14475,\n",
       " 284,\n",
       " 606,\n",
       " 11,\n",
       " 407,\n",
       " 5101,\n",
       " 11,\n",
       " 1276,\n",
       " 1037,\n",
       " 13,\n",
       " 978,\n",
       " 441,\n",
       " 11,\n",
       " 198,\n",
       " 1639,\n",
       " 389,\n",
       " 18665,\n",
       " 416,\n",
       " 35765,\n",
       " 414,\n",
       " 198,\n",
       " 817,\n",
       " 1555,\n",
       " 810,\n",
       " 517,\n",
       " 32743,\n",
       " 345,\n",
       " 11,\n",
       " 290,\n",
       " 345,\n",
       " 47397,\n",
       " 198,\n",
       " 464,\n",
       " 932,\n",
       " 907,\n",
       " 267,\n",
       " 6,\n",
       " 262,\n",
       " 1181,\n",
       " 11,\n",
       " 508,\n",
       " 1337,\n",
       " 329,\n",
       " 345,\n",
       " 588,\n",
       " 17150,\n",
       " 11,\n",
       " 198,\n",
       " 2215,\n",
       " 345,\n",
       " 17328,\n",
       " 606,\n",
       " 355,\n",
       " 5775,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 5962,\n",
       " 22307,\n",
       " 25,\n",
       " 198,\n",
       " 17784,\n",
       " 329,\n",
       " 514,\n",
       " 0,\n",
       " 6407,\n",
       " 11,\n",
       " 5600,\n",
       " 0,\n",
       " 1119,\n",
       " 497,\n",
       " 6,\n",
       " 263,\n",
       " 19951,\n",
       " 329,\n",
       " 514,\n",
       " 198,\n",
       " 25907,\n",
       " 25,\n",
       " 8659,\n",
       " 514,\n",
       " 284,\n",
       " 1145,\n",
       " 680,\n",
       " 11,\n",
       " 290,\n",
       " 511,\n",
       " 3650,\n",
       " 12,\n",
       " 20089,\n",
       " 198,\n",
       " 66,\n",
       " 859,\n",
       " 1150,\n",
       " 351,\n",
       " 13020,\n",
       " 26,\n",
       " 787,\n",
       " 1225,\n",
       " 14137,\n",
       " 329,\n",
       " 514,\n",
       " 1601,\n",
       " 11,\n",
       " 284,\n",
       " 198,\n",
       " 11284,\n",
       " 514,\n",
       " 17496,\n",
       " 26,\n",
       " 14634,\n",
       " 4445,\n",
       " 597,\n",
       " 17950,\n",
       " 462,\n",
       " 719,\n",
       " 198,\n",
       " 27718,\n",
       " 1028,\n",
       " 262,\n",
       " 5527,\n",
       " 11,\n",
       " 290,\n",
       " 2148,\n",
       " 517,\n",
       " 198,\n",
       " 79,\n",
       " 959,\n",
       " 2259,\n",
       " 24895,\n",
       " 4445,\n",
       " 11,\n",
       " 284,\n",
       " 6333,\n",
       " 510,\n",
       " 290,\n",
       " 39300,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.d_k = config.n_embd // config.n_head\n",
    "        self.scale = self.d_k ** -0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.n_head, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "\n",
    "        attn_output = (attn_probs @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        attn_output = self.resid_dropout(self.out_proj(attn_output))\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None, \"Config must include vocab_size\"\n",
    "        assert config.block_size is not None, \"Config must include block_size\"\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'drop': nn.Dropout(config.dropout),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=1e-5),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.init_residuals()\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def init_residuals(self):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = logits[:, [-1], :]  # Use only the last token's logits\n",
    "            return logits, None\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}, \"Invalid model type\"\n",
    "        override_args = override_args or {}\n",
    "        assert all(k == 'dropout' for k in override_args), \"Only 'dropout' can be overridden\"\n",
    "\n",
    "        print(f\"Loading weights from pretrained model: {model_type}\")\n",
    "\n",
    "        config_args = cls.get_config_args(model_type)\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_pretrained_weights(model_type)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_args(model_type):\n",
    "        config_map = {\n",
    "            'gpt2': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'gpt2-medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'gpt2-large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'gpt2-xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
    "        }\n",
    "        config_args = config_map[model_type]\n",
    "        config_args.update({'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'dropout': 0.1})\n",
    "        return config_args\n",
    "\n",
    "    def load_pretrained_weights(self, model_type):\n",
    "        model_hf = transformers.GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd = self.state_dict()\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for k, v in sd_hf.items():\n",
    "            if k in sd:\n",
    "                if any(k.endswith(w) for w in transposed):\n",
    "                    sd[k].copy_(v.t())\n",
    "                else:\n",
    "                    sd[k].copy_(v)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        N = self.get_num_params()\n",
    "        L, H, Q, T = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.block_size\n",
    "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        flops_achieved = flops_per_iter * (1.0 / dt)\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257, \n",
    "    block_size=1024, \n",
    "    n_layer=12, \n",
    "    n_head=12, \n",
    "    n_embd=768, \n",
    "    dropout=0.1, \n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "customGPT = GPT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dependencies\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import nullcontext\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgloo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# System\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     43\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mcompile\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Disable compilation for now as PyTorch 2.0 is not yet stable on all devices\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Default config values designed to train a GPT-2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# Data\n",
    "# data_path = 'data/openwebtext.txt'  # Path to your text file\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 12\n",
    "block_size = 1024\n",
    "\n",
    "# Model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer\n",
    "learning_rate = 6e-4\n",
    "max_iters = 600000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = 600000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'gloo'\n",
    "\n",
    "# System\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "dtype = 'float32'  # MPS currently supports only float32\n",
    "compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Collect configuration keys\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "\n",
    "# Various initializations and derived attributes\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n",
    "    torch.mps.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    seed_offset = ddp_rank\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'mps' if 'mps' in device else 'cpu'\n",
    "ptdtype = torch.float32  # Currently, MPS supports only float32\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(n_layer\u001b[38;5;241m=\u001b[39mn_layer, n_head\u001b[38;5;241m=\u001b[39mn_head, n_embd\u001b[38;5;241m=\u001b[39mn_embd, block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m----> 7\u001b[0m                   bias\u001b[38;5;241m=\u001b[39mbias, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtokens), dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_from \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscratch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing a new model from scratch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize iteration number and best validation loss\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# Model initialization\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GradScaler\n",
    "scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # Free up memory\n",
    "\n",
    "# Wrap model into DDP container if needed\n",
    "if ddp:\n",
    "    print(f\"Starting parallel process with rank {ddp_rank}\")\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# Function to estimate loss over splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n",
    "            for k in range(eval_iters):\n",
    "                batch = next(iter(dataloader))\n",
    "                X, Y = batch['input'].to(device), batch['target'].to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                pbar.update(1)\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 200/200 [04:33<00:00,  1.37s/it]\n",
      "Evaluating val: 100%|██████████| 200/200 [04:57<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 12.9407, val loss 12.9414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0: 100%|██████████| 40/40 [18:01<00:00, 27.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 12.9594, time 1656955.16ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1:   0%|          | 0/40 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.47 GB, other allocations: 1006.20 MB, max allowed: 18.13 GB). Tried to allocate 2.01 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 37\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m     39\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 15.47 GB, other allocations: 1006.20 MB, max allowed: 18.13 GB). Tried to allocate 2.01 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "raw_model = model.module if ddp else model\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "t0 = time.time()\n",
    "\n",
    "while iter_num <= max_iters:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = batch['input'].to(device), batch['target'].to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            pbar.update(1)\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
