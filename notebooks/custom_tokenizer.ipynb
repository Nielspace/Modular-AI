{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "import tiktoken\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'tiny_shakespeare.txt'\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_file_path = os.path.join(data_dir, input_file)\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, model=\"gpt2\", seq_length=400):\n",
    "        tokenizer = tiktoken.get_encoding(model)\n",
    "        self.tokens = tokenizer.encode(data)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.x, self.y = self.create_sequences()\n",
    "\n",
    "    def create_sequences(self):\n",
    "        x, y = [], []\n",
    "        for i in range(0, len(self.tokens) - self.seq_length, self.seq_length):\n",
    "            x.append(self.tokens[i:i+self.seq_length])\n",
    "            y.append(self.tokens[i+1:i+1+self.seq_length])\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_seq = torch.tensor(self.x[idx], dtype=torch.long)\n",
    "        target_seq = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        sample = {'input': input_seq, 'target': target_seq}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(batch):\n",
    "    input_seqs = [item['input'] for item in batch]\n",
    "    target_seqs = [item['target'] for item in batch]\n",
    "\n",
    "    input_padded = torch.nn.utils.rnn.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_padded = torch.nn.utils.rnn.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'input': input_padded, 'target': target_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.d_k = config.n_embd // config.n_head\n",
    "        self.scale = self.d_k ** -0.5\n",
    "\n",
    "        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.n_head, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "\n",
    "        attn_output = (attn_probs @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        attn_output = self.resid_dropout(self.out_proj(attn_output))\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None, \"Config must include vocab_size\"\n",
    "        assert config.block_size is not None, \"Config must include block_size\"\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'drop': nn.Dropout(config.dropout),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=1e-5),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.init_residuals()\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def init_residuals(self):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = logits[:, [-1], :]  # Use only the last token's logits\n",
    "            return logits, None\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}, \"Invalid model type\"\n",
    "        override_args = override_args or {}\n",
    "        assert all(k == 'dropout' for k in override_args), \"Only 'dropout' can be overridden\"\n",
    "\n",
    "        print(f\"Loading weights from pretrained model: {model_type}\")\n",
    "\n",
    "        config_args = cls.get_config_args(model_type)\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_pretrained_weights(model_type)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_args(model_type):\n",
    "        config_map = {\n",
    "            'gpt2': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'gpt2-medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'gpt2-large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'gpt2-xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
    "        }\n",
    "        config_args = config_map[model_type]\n",
    "        config_args.update({'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'dropout': 0.1})\n",
    "        return config_args\n",
    "\n",
    "    def load_pretrained_weights(self, model_type):\n",
    "        model_hf = transformers.GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd = self.state_dict()\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for k, v in sd_hf.items():\n",
    "            if k in sd:\n",
    "                if any(k.endswith(w) for w in transposed):\n",
    "                    sd[k].copy_(v.t())\n",
    "                else:\n",
    "                    sd[k].copy_(v)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        N = self.get_num_params()\n",
    "        L, H, Q, T = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.block_size\n",
    "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        flops_achieved = flops_per_iter * (1.0 / dt)\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257, \n",
    "    block_size=1024, \n",
    "    n_layer=12, \n",
    "    n_head=12, \n",
    "    n_embd=768, \n",
    "    dropout=0.1, \n",
    "    bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "customGPT = GPT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dependencies\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import nullcontext\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgloo\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# System\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     43\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mcompile\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Disable compilation for now as PyTorch 2.0 is not yet stable on all devices\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Default config values designed to train a GPT-2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# Data\n",
    "# data_path = 'data/openwebtext.txt'  # Path to your text file\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 12\n",
    "block_size = 1024\n",
    "\n",
    "# Model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer\n",
    "learning_rate = 6e-4\n",
    "max_iters = 600000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = 600000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'gloo'\n",
    "\n",
    "# System\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "dtype = 'float32'  # MPS currently supports only float32\n",
    "compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Collect configuration keys\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "\n",
    "# Various initializations and derived attributes\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n",
    "    torch.mps.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    seed_offset = ddp_rank\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'mps' if 'mps' in device else 'cpu'\n",
    "ptdtype = torch.float32  # Currently, MPS supports only float32\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(n_layer\u001b[38;5;241m=\u001b[39mn_layer, n_head\u001b[38;5;241m=\u001b[39mn_head, n_embd\u001b[38;5;241m=\u001b[39mn_embd, block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m----> 7\u001b[0m                   bias\u001b[38;5;241m=\u001b[39mbias, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtokens), dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_from \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscratch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing a new model from scratch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize iteration number and best validation loss\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# Model initialization\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GradScaler\n",
    "scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # Free up memory\n",
    "\n",
    "# Wrap model into DDP container if needed\n",
    "if ddp:\n",
    "    print(f\"Starting parallel process with rank {ddp_rank}\")\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# Function to estimate loss over splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n",
    "            for k in range(eval_iters):\n",
    "                batch = next(iter(dataloader))\n",
    "                X, Y = batch['input'].to(device), batch['target'].to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                pbar.update(1)\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 200/200 [04:33<00:00,  1.37s/it]\n",
      "Evaluating val: 100%|██████████| 200/200 [04:57<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 12.9407, val loss 12.9414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0: 100%|██████████| 40/40 [18:01<00:00, 27.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 12.9594, time 1656955.16ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1:   0%|          | 0/40 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.47 GB, other allocations: 1006.20 MB, max allowed: 18.13 GB). Tried to allocate 2.01 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 37\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m     39\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 15.47 GB, other allocations: 1006.20 MB, max allowed: 18.13 GB). Tried to allocate 2.01 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "raw_model = model.module if ddp else model\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "t0 = time.time()\n",
    "\n",
    "while iter_num <= max_iters:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = batch['input'].to(device), batch['target'].to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            pbar.update(1)\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
