{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "from einops import rearrange\n",
    "from typing import Optional, Union\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb_log = True  \n",
    "run_name = 'Sparse Attention - mps'\n",
    "project_name = \"Attention Benchmark\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, articles, model=\"gpt2\", seq_length=512):\n",
    "        self.tokenizer = tiktoken.get_encoding(model)\n",
    "        self.vocab_size = self.tokenizer.n_vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.articles = articles.apply(self.preprocess_and_tokenize)\n",
    "        \n",
    "        self.input_ids, self.targets, self.attention_masks = self.create_sequences()\n",
    "\n",
    "    def preprocess_and_tokenize(self, text):\n",
    "        # Preprocess text\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize text\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        # Check for invalid token indices\n",
    "        assert all(token < self.vocab_size for token in tokens), \"Token index out of range\"\n",
    "        \n",
    "        # Pad and truncate tokens\n",
    "        if len(tokens) > self.seq_length:\n",
    "            tokens = tokens[:self.seq_length]\n",
    "        else:\n",
    "            tokens += [0] * (self.seq_length - len(tokens))\n",
    "        self.tokens = tokens\n",
    "        return tokens\n",
    "\n",
    "    def create_sequences(self):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        targets = []\n",
    "        \n",
    "        for tokens in self.articles:\n",
    "            input_ids.append(tokens[:-1])  # Exclude the last token for input\n",
    "            targets.append(tokens[1:])     # Exclude the first token for target\n",
    "            attention_masks.append([1 if token != 0 else 0 for token in tokens[:-1]])\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, targets, attention_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        input_seq = self.input_ids[idx]\n",
    "        attention_mask = self.attention_masks[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        \n",
    "        sample = {'input_ids': input_seq, 'targets': target_seq, 'attention_mask': attention_mask}\n",
    "        return sample\n",
    "\n",
    "def pad_sequences(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    targets = [item['targets'] for item in batch]\n",
    "\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    targets_padded = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {'input_ids': input_ids_padded, 'targets': targets_padded, 'attention_mask': attention_masks_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "df = pd.read_parquet(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")\n",
    "articles = df.iloc[:, 1]\n",
    "\n",
    "dataset = TextDataset(articles, model=\"gpt2\", seq_length=512)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/kyegomez/SparseAttention/blob/main/sparse_attention.py\n",
    "\n",
    "def get_attn_mask(n, attn_mode, local_attn_ctx=None):\n",
    "    if attn_mode == 'all':\n",
    "        b = torch.tril(torch.ones([n, n]))\n",
    "    elif attn_mode == 'local':\n",
    "        bandwidth = local_attn_ctx\n",
    "        ctx = min(n - 1, bandwidth - 1)\n",
    "        b = torch.tril(torch.ones([n, n]), ctx)\n",
    "    elif attn_mode == 'strided':\n",
    "        stride = local_attn_ctx\n",
    "        x = torch.reshape(torch.arange(n, dtype=torch.int32), [n, 1])\n",
    "        y = torch.transpose(x, 0, 1)\n",
    "        z = torch.zeros([n, n], dtype=torch.int32)\n",
    "        q = z + x\n",
    "        k = z + y\n",
    "        c1 = q >= k\n",
    "        c2 = torch.eq(torch.fmod(q - k, stride), 0)\n",
    "        c3 = torch.logical_and(c1, c2)\n",
    "        b = c3.float()\n",
    "    else:\n",
    "        raise ValueError('Not yet implemented')\n",
    "    b = torch.reshape(b, [1, 1, n, n])\n",
    "    return b\n",
    "\n",
    "def strided_transpose(x, n_ctx, local_attn_ctx, blocksize):\n",
    "    bT_ctx = n_ctx // local_attn_ctx\n",
    "    assert bT_ctx % blocksize == 0, f'{bT_ctx}, {blocksize}'\n",
    "    n, t, embd = x.size()\n",
    "    x = torch.reshape(x, [n, bT_ctx, local_attn_ctx, embd])\n",
    "    x = torch.transpose(x, 1, 2)\n",
    "    x = torch.reshape(x, [n, t, embd])\n",
    "    return x\n",
    "\n",
    "def split_heads(x, n):\n",
    "    return torch.transpose(split_states(x, n), 1, 2)\n",
    "\n",
    "def merge_heads(x):\n",
    "    return merge_states(torch.transpose(x, 1, 2))\n",
    "\n",
    "def split_states(x, n):\n",
    "    x_shape = x.size()\n",
    "    m = x_shape[-1]\n",
    "    new_x_shape = x_shape[:-1] + [n, m // n]\n",
    "    return torch.reshape(x, new_x_shape)\n",
    "\n",
    "def merge_states(x):\n",
    "    x_shape = x.size()\n",
    "    new_x_shape = x_shape[:-2] + [np.prod(x_shape[-2:])]\n",
    "    return torch.reshape(x, new_x_shape)\n",
    "\n",
    "def blocksparse_attention_impl(q, k, v, heads, attn_mode, local_attn_ctx=None, blocksize=32):\n",
    "    n_ctx = q.size()[1]\n",
    "    if attn_mode == 'strided':\n",
    "        q = strided_transpose(q, n_ctx, local_attn_ctx, blocksize)\n",
    "        k = strided_transpose(k, n_ctx, local_attn_ctx, blocksize)\n",
    "        v = strided_transpose(v, n_ctx, local_attn_ctx, blocksize)\n",
    "    n_state = q.size()[-1] // heads\n",
    "    scale_amount = 1.0 / np.sqrt(n_state)\n",
    "    w = torch.matmul(q, k.transpose(-2, -1))\n",
    "    w = F.softmax(w * scale_amount, dim=-1)\n",
    "    a = torch.matmul(w, v)\n",
    "    if attn_mode == 'strided':\n",
    "        n, t, embd = a.size()\n",
    "        bT_ctx = n_ctx // local_attn_ctx\n",
    "        a = torch.reshape(a, [n, local_attn_ctx, bT_ctx, embd])\n",
    "        a = torch.transpose(a, 1, 2)\n",
    "        a = torch.reshape(a, [n, t, embd])\n",
    "    return a\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    # n_batch = 4\n",
    "    # n_ctx = 1024\n",
    "    # n_embd = 256\n",
    "    # heads = 4\n",
    "    # attn_mode = \"all\"\n",
    "    # local_attn_ctx = 32\n",
    "    # blocksize = 32\n",
    "    def __init__(self, config):\n",
    "        super(SparseAttention, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.dropout_p = config.dropout\n",
    "        self.blocksize = config.block_size\n",
    "        self.local_attn_ctx = 32\n",
    "        self.attn_mode = \"all\"\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        qkv = self.qkv(x).view(b, t, 3, self.n_head, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        q, k, v = [rearrange(x, 'b t h d -> (b h) t d') for x in (q, k, v)]\n",
    "\n",
    "        # Implement sparse attention logic\n",
    "        attn_output = blocksparse_attention_impl(q, k, v, self.n_head, self.attn_mode, self.local_attn_ctx, self.blocksize)\n",
    "        attn_output = rearrange(attn_output, '(b h) t d -> b t (h d)', h=self.n_head)\n",
    "        attn_output = self.proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.attn = SparseAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Assuming GPTConfig is defined somewhere in the code\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None, \"Config must include vocab_size\"\n",
    "        assert config.block_size is not None, \"Config must include block_size\"\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'drop': nn.Dropout(config.dropout),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=1e-5),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.init_residuals()\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def init_residuals(self):\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = logits[:, [-1], :]  # Use only the last token's logits\n",
    "            return logits, None\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}, \"Invalid model type\"\n",
    "        override_args = override_args or {}\n",
    "        assert all(k == 'dropout' for k in override_args), \"Only 'dropout' can be overridden\"\n",
    "\n",
    "        print(f\"Loading weights from pretrained model: {model_type}\")\n",
    "\n",
    "        config_args = cls.get_config_args(model_type)\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        model.load_pretrained_weights(model_type)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_args(model_type):\n",
    "        config_map = {\n",
    "            'gpt2': {'n_layer': 12, 'n_head': 12, 'n_embd': 768},\n",
    "            'gpt2-medium': {'n_layer': 24, 'n_head': 16, 'n_embd': 1024},\n",
    "            'gpt2-large': {'n_layer': 36, 'n_head': 20, 'n_embd': 1280},\n",
    "            'gpt2-xl': {'n_layer': 48, 'n_head': 25, 'n_embd': 1600},\n",
    "        }\n",
    "        config_args = config_map[model_type]\n",
    "        config_args.update({'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'dropout': 0.1})\n",
    "        return config_args\n",
    "\n",
    "    def load_pretrained_weights(self, model_type):\n",
    "        model_hf = transformers.GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd = self.state_dict()\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        for k, v in sd_hf.items():\n",
    "            if k in sd:\n",
    "                if any(k.endswith(w) for w in transposed):\n",
    "                    sd[k].copy_(v.t())\n",
    "                else:\n",
    "                    sd[k].copy_(v)\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        N = self.get_num_params()\n",
    "        L, H, Q, T = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.block_size\n",
    "        flops_per_token = 6 * N + 12 * L * H * Q * T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        flops_achieved = flops_per_iter * (1.0 / dt)\n",
    "        flops_promised = 312e12\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dependencies\n",
    "import numpy as np\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import nullcontext\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per iteration will be: 40,960\n"
     ]
    }
   ],
   "source": [
    "# Default config values designed to train a GPT-2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# Data\n",
    "# data_path = 'data/openwebtext.txt'  # Path to your text file\n",
    "gradient_accumulation_steps = 5 * 8\n",
    "batch_size = 2\n",
    "block_size = 512\n",
    "\n",
    "# Model\n",
    "n_layer = 6  # Reduce the number of layers\n",
    "n_head = 8   # Reduce the number of attention heads\n",
    "n_embd = 512 # Reduce the embedding size\n",
    "dropout = 0.0\n",
    "bias = False\n",
    "\n",
    "# AdamW optimizer\n",
    "learning_rate = 6e-4\n",
    "max_iters = 1000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 200\n",
    "lr_decay_iters = 1000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'gloo'\n",
    "\n",
    "# System\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "dtype = 'float32'  # MPS currently supports only float32\n",
    "compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Collect configuration keys\n",
    "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}\n",
    "\n",
    "# Various initializations and derived attributes\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n",
    "    torch.mps.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    seed_offset = ddp_rank\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = 'mps' if 'mps' in device else 'cpu'\n",
    "ptdtype = torch.float32  # Currently, MPS supports only float32\n",
    "ctx = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "Number of parameters: 19.18M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(512, 512)\n",
       "    (wpe): Embedding(512, 512)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): SparseAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        \n",
    "# Initialize iteration number and best validation loss\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# Model initialization\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "elif init_from == 'resume':\n",
    "    print(f\"Resuming training from {out_dir}\")\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gptconf = GPTConfig(**model_args)\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "elif init_from.startswith('gpt2'):\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "    override_args = dict(dropout=dropout)\n",
    "    model = GPT.from_pretrained(init_from, override_args)\n",
    "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a GradScaler\n",
    "scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None  # Free up memory\n",
    "\n",
    "# Wrap model into DDP container if needed\n",
    "if ddp:\n",
    "    print(f\"Starting parallel process with rank {ddp_rank}\")\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "\n",
    "# Function to estimate loss over splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n",
    "            for k in range(eval_iters):\n",
    "                batch = next(iter(dataloader))\n",
    "                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "                pbar.update(1)\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnielspace\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588fa425e6c84175980fa2ed364e7dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168043511111137, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nielspace/Documents/Deep Learning/DL-research/NNResearch/notebooks/wandb/run-20240708_145713-0plukv62</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nielspace/Attention%20Benchmark/runs/0plukv62' target=\"_blank\">Sparse Attention - mps</a></strong> to <a href='https://wandb.ai/nielspace/Attention%20Benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nielspace/Attention%20Benchmark' target=\"_blank\">https://wandb.ai/nielspace/Attention%20Benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nielspace/Attention%20Benchmark/runs/0plukv62' target=\"_blank\">https://wandb.ai/nielspace/Attention%20Benchmark/runs/0plukv62</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating train: 100%|██████████| 200/200 [00:11<00:00, 17.95it/s]\n",
      "Evaluating val: 100%|██████████| 200/200 [00:11<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.0792, val loss 2.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0: 100%|██████████| 40/40 [00:06<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 2.0792, time 28972.68ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1: 100%|██████████| 40/40 [00:06<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: loss 2.0792, time 6287.87ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2: 100%|██████████| 40/40 [00:06<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2: loss 2.0515, time 6272.93ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3: loss 1.9979, time 6204.72ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 4: 100%|██████████| 40/40 [00:06<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4: loss 1.9236, time 6341.40ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 5: 100%|██████████| 40/40 [00:06<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5: loss 1.8395, time 6309.09ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 6: 100%|██████████| 40/40 [00:06<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6: loss 1.7573, time 6345.16ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 7: 100%|██████████| 40/40 [00:06<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7: loss 1.6839, time 6334.63ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 8: 100%|██████████| 40/40 [00:06<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8: loss 1.6196, time 6315.10ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 9: 100%|██████████| 40/40 [00:06<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9: loss 1.5624, time 6257.82ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 10: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10: loss 1.5099, time 6211.21ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 11: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 11: loss 1.4613, time 6206.24ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 12: 100%|██████████| 40/40 [00:06<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 12: loss 1.4177, time 6271.60ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 13: 100%|██████████| 40/40 [00:06<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 13: loss 1.3793, time 6297.36ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 14: 100%|██████████| 40/40 [00:06<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 14: loss 1.3436, time 6323.59ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 15: 100%|██████████| 40/40 [00:06<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 15: loss 1.3084, time 6313.74ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 16: 100%|██████████| 40/40 [00:06<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 16: loss 1.2727, time 6279.07ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 17: 100%|██████████| 40/40 [00:06<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 17: loss 1.2340, time 6390.66ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 18: 100%|██████████| 40/40 [00:06<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 18: loss 1.1878, time 6504.18ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 19: 100%|██████████| 40/40 [00:06<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 19: loss 1.1325, time 6385.13ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 20: 100%|██████████| 40/40 [00:06<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 20: loss 1.0673, time 6548.14ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 21: 100%|██████████| 40/40 [00:06<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 21: loss 0.9927, time 6590.30ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 22: 100%|██████████| 40/40 [00:06<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 22: loss 0.9138, time 6501.31ms, mfu 0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 23: 100%|██████████| 40/40 [00:06<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 23: loss 0.8300, time 6677.75ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 24: 100%|██████████| 40/40 [00:07<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 24: loss 0.7440, time 7144.75ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 25: 100%|██████████| 40/40 [00:06<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 25: loss 0.6613, time 6627.22ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 26: 100%|██████████| 40/40 [00:06<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 26: loss 0.5874, time 6999.51ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 27: 100%|██████████| 40/40 [00:06<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 27: loss 0.5171, time 7220.75ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 28: 100%|██████████| 40/40 [00:06<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 28: loss 0.4516, time 6750.00ms, mfu 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 29: 100%|██████████| 40/40 [00:06<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 29: loss 0.3904, time 7065.47ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 30: 100%|██████████| 40/40 [00:06<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 30: loss 0.3320, time 6870.68ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 31: 100%|██████████| 40/40 [00:06<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 31: loss 0.2788, time 7070.66ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 32: 100%|██████████| 40/40 [00:06<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 32: loss 0.2312, time 7043.51ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 33: 100%|██████████| 40/40 [00:07<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 33: loss 0.1896, time 7363.56ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 34: 100%|██████████| 40/40 [00:07<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 34: loss 0.1541, time 7342.09ms, mfu 0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 35: 100%|██████████| 40/40 [00:07<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35: loss 0.1248, time 7641.67ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 36: 100%|██████████| 40/40 [00:06<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 36: loss 0.1008, time 7085.20ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 37: 100%|██████████| 40/40 [00:07<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 37: loss 0.0815, time 7303.90ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 38: 100%|██████████| 40/40 [00:07<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 38: loss 0.0671, time 7407.49ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 39: 100%|██████████| 40/40 [00:07<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 39: loss 0.0556, time 7211.87ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 40: 100%|██████████| 40/40 [00:07<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 40: loss 0.0461, time 7468.69ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 41: 100%|██████████| 40/40 [00:07<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 41: loss 0.0387, time 7503.11ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 42: 100%|██████████| 40/40 [00:07<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 42: loss 0.0327, time 7272.55ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 43: 100%|██████████| 40/40 [00:07<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 43: loss 0.0279, time 7452.56ms, mfu 0.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 44: 100%|██████████| 40/40 [00:07<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 44: loss 0.0243, time 7443.07ms, mfu 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 45: 100%|██████████| 40/40 [00:07<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 45: loss 0.0212, time 7766.73ms, mfu 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 46: 100%|██████████| 40/40 [00:07<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 46: loss 0.0188, time 7826.88ms, mfu 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 47: 100%|██████████| 40/40 [00:07<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 47: loss 0.0167, time 7856.36ms, mfu 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 48: 100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 48: loss 0.0149, time 8145.17ms, mfu 0.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 49: 100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 49: loss 0.0134, time 8135.21ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 50: 100%|██████████| 40/40 [00:08<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 50: loss 0.0122, time 8311.11ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 51: 100%|██████████| 40/40 [00:08<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 51: loss 0.0111, time 8522.85ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 52: 100%|██████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 52: loss 0.0102, time 8478.73ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 53: 100%|██████████| 40/40 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 53: loss 0.0094, time 8483.76ms, mfu 0.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 54: 100%|██████████| 40/40 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 54: loss 0.0087, time 8692.19ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 55: 100%|██████████| 40/40 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 55: loss 0.0081, time 8823.55ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 56: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 56: loss 0.0075, time 9210.20ms, mfu 0.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 57: 100%|██████████| 40/40 [00:09<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 57: loss 0.0070, time 9519.71ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 58: 100%|██████████| 40/40 [00:09<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 58: loss 0.0066, time 9279.11ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 59: 100%|██████████| 40/40 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 59: loss 0.0062, time 8736.87ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 60: 100%|██████████| 40/40 [00:08<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 60: loss 0.0058, time 8558.49ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 61: 100%|██████████| 40/40 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 61: loss 0.0055, time 8657.40ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 62: 100%|██████████| 40/40 [00:08<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 62: loss 0.0052, time 8604.86ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 63: 100%|██████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 63: loss 0.0049, time 8523.15ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 64: 100%|██████████| 40/40 [00:08<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 64: loss 0.0047, time 8438.62ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 65: 100%|██████████| 40/40 [00:08<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 65: loss 0.0045, time 8507.97ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 66: 100%|██████████| 40/40 [00:08<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 66: loss 0.0043, time 8409.17ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 67: 100%|██████████| 40/40 [00:07<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 67: loss 0.0041, time 8175.20ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 68: 100%|██████████| 40/40 [00:08<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 68: loss 0.0039, time 8235.49ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 69: 100%|██████████| 40/40 [00:08<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69: loss 0.0037, time 8297.78ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 70: 100%|██████████| 40/40 [00:07<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 70: loss 0.0036, time 8105.12ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 71: 100%|██████████| 40/40 [00:07<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 71: loss 0.0035, time 8145.36ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 72: 100%|██████████| 40/40 [00:07<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 72: loss 0.0033, time 8176.21ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 73: 100%|██████████| 40/40 [00:08<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 73: loss 0.0032, time 8309.55ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 74: 100%|██████████| 40/40 [00:08<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 74: loss 0.0031, time 8228.12ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 75: 100%|██████████| 40/40 [00:08<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 75: loss 0.0030, time 8401.03ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 76: 100%|██████████| 40/40 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 76: loss 0.0029, time 8552.22ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 77: 100%|██████████| 40/40 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 77: loss 0.0028, time 8563.10ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 78: 100%|██████████| 40/40 [00:08<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 78: loss 0.0027, time 8593.52ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 79: 100%|██████████| 40/40 [00:08<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 79: loss 0.0026, time 8584.49ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 80: 100%|██████████| 40/40 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 80: loss 0.0025, time 8734.01ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 81: 100%|██████████| 40/40 [00:08<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 81: loss 0.0025, time 8667.93ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 82: 100%|██████████| 40/40 [00:08<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 82: loss 0.0024, time 8867.31ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 83: 100%|██████████| 40/40 [00:08<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 83: loss 0.0023, time 8968.49ms, mfu 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 84: 100%|██████████| 40/40 [00:08<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 84: loss 0.0022, time 8960.36ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 85: 100%|██████████| 40/40 [00:08<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 85: loss 0.0022, time 8849.45ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 86: 100%|██████████| 40/40 [00:08<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 86: loss 0.0021, time 8985.99ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 87: 100%|██████████| 40/40 [00:09<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 87: loss 0.0021, time 9281.66ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 88: 100%|██████████| 40/40 [00:08<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 88: loss 0.0020, time 9053.98ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 89: 100%|██████████| 40/40 [00:08<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 89: loss 0.0020, time 9090.53ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 90: 100%|██████████| 40/40 [00:09<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 90: loss 0.0019, time 9396.01ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 91: 100%|██████████| 40/40 [00:09<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 91: loss 0.0019, time 9397.36ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 92: 100%|██████████| 40/40 [00:08<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 92: loss 0.0018, time 9217.86ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 93: 100%|██████████| 40/40 [00:09<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 93: loss 0.0018, time 9311.67ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 94: 100%|██████████| 40/40 [00:09<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 94: loss 0.0017, time 9532.29ms, mfu 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 95: 100%|██████████| 40/40 [00:09<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 95: loss 0.0017, time 9369.97ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 96: 100%|██████████| 40/40 [00:08<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 96: loss 0.0016, time 9060.36ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 97: 100%|██████████| 40/40 [00:09<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 97: loss 0.0016, time 9536.89ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 98: 100%|██████████| 40/40 [00:09<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 98: loss 0.0016, time 9702.55ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 99: 100%|██████████| 40/40 [00:09<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 99: loss 0.0015, time 9404.45ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 100: 100%|██████████| 40/40 [00:09<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100: loss 0.0015, time 9433.11ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 101: 100%|██████████| 40/40 [00:09<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 101: loss 0.0015, time 9796.40ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 102: 100%|██████████| 40/40 [00:09<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 102: loss 0.0014, time 10085.78ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 103: 100%|██████████| 40/40 [00:09<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 103: loss 0.0014, time 9788.30ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 104: 100%|██████████| 40/40 [00:09<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 104: loss 0.0014, time 9855.76ms, mfu 0.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 105: 100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 105: loss 0.0013, time 10510.60ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 106: 100%|██████████| 40/40 [00:09<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 106: loss 0.0013, time 10091.88ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 107: 100%|██████████| 40/40 [00:09<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 107: loss 0.0013, time 9841.14ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 108: 100%|██████████| 40/40 [00:09<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 108: loss 0.0012, time 10072.41ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 109: 100%|██████████| 40/40 [00:10<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 109: loss 0.0012, time 10284.20ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 110: 100%|██████████| 40/40 [00:09<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 110: loss 0.0012, time 10088.13ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 111: 100%|██████████| 40/40 [00:09<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 111: loss 0.0012, time 9932.86ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 112: 100%|██████████| 40/40 [00:10<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 112: loss 0.0011, time 10393.79ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 113: 100%|██████████| 40/40 [00:10<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 113: loss 0.0011, time 10283.37ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 114: 100%|██████████| 40/40 [00:09<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 114: loss 0.0011, time 10017.36ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 115: 100%|██████████| 40/40 [00:10<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 115: loss 0.0011, time 10390.88ms, mfu 0.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 116: 100%|██████████| 40/40 [00:12<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 116: loss 0.0010, time 12845.46ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 117: 100%|██████████| 40/40 [00:10<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 117: loss 0.0010, time 10693.46ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 118: 100%|██████████| 40/40 [00:10<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 118: loss 0.0010, time 10456.20ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 119: 100%|██████████| 40/40 [00:10<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 119: loss 0.0010, time 11175.97ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 120: 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 120: loss 0.0009, time 11126.87ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 121: 100%|██████████| 40/40 [00:10<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 121: loss 0.0009, time 10614.73ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 122: 100%|██████████| 40/40 [00:10<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 122: loss 0.0009, time 10664.28ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 123: 100%|██████████| 40/40 [00:10<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 123: loss 0.0009, time 11025.47ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 124: 100%|██████████| 40/40 [00:10<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 124: loss 0.0008, time 10613.13ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 125: 100%|██████████| 40/40 [00:10<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 125: loss 0.0008, time 10718.66ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 126: 100%|██████████| 40/40 [00:10<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 126: loss 0.0008, time 10897.63ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 127: 100%|██████████| 40/40 [00:10<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 127: loss 0.0008, time 11119.12ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 128: 100%|██████████| 40/40 [00:10<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 128: loss 0.0008, time 10600.86ms, mfu 0.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 129: 100%|██████████| 40/40 [00:10<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 129: loss 0.0007, time 11061.35ms, mfu 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 130: 100%|██████████| 40/40 [00:10<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 130: loss 0.0007, time 11144.56ms, mfu 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 131: 100%|██████████| 40/40 [00:10<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 131: loss 0.0007, time 10753.83ms, mfu 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 132: 100%|██████████| 40/40 [00:10<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 132: loss 0.0007, time 10814.37ms, mfu 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 133: 100%|██████████| 40/40 [00:12<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 133: loss 0.0007, time 13238.06ms, mfu 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 134:  62%|██████▎   | 25/40 [00:08<00:05,  2.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync \u001b[38;5;241m=\u001b[39m (micro_step \u001b[38;5;241m==\u001b[39m gradient_accumulation_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[0;32m---> 55\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     57\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=project_name, name= run_name, config=config)\n",
    "\n",
    "raw_model = model.module if ddp else model\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "t0 = time.time()\n",
    "\n",
    "while iter_num <= max_iters:\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train_loss\": losses['train'],\n",
    "                \"val_loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "            })\n",
    "\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n",
    "        total_train_loss = 0.0  # Initialize total training loss\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            batch = next(iter(dataloader))\n",
    "            X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            pbar.update(1)\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train_loss\": total_train_loss,\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "            })\n",
    "\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
