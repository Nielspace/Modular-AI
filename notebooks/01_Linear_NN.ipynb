{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fe5df4-5903-4818-8d72-8d3571597c6d",
   "metadata": {},
   "source": [
    "# Linear Neural Networks\n",
    "\n",
    "In this notebook, we will explore the mathematical foundations and implementation of Linear Neural Networks. Linear models are the simplest form of neural networks and are primarily used for linear regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158649f-a43b-453e-85a2-a538c1b4f8e0",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Overview\n",
    "Linear Neural Networks are composed of layers where each neuron performs a linear transformation of the input.\n",
    "\n",
    "**Type of Function**: Linear\n",
    "\n",
    "**Nature**: Continuous\n",
    "\n",
    "**Behavior**: Linear neural networks are the simplest form of neural networks where the output is a linear combination of the input features. \n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "\n",
    "The output \\(y\\) is given by:\n",
    "\\[ y = XW + b1\\]\n",
    "where \\( X \\) is the input, \\( W \\) is the weight matrix, and \\( b \\) is the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e360f8-102b-4cac-956b-47ef5d687196",
   "metadata": {},
   "source": [
    "# Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b117ea2a-afc1-4b6f-8afb-07eb614baa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc1eb8-3f29-469d-a000-58bb680e14bd",
   "metadata": {},
   "source": [
    "### Mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73be90fe-5e43-4bdc-9afb-d72b6082622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) =  17.0\n",
      "df/dx =  8.0\n"
     ]
    }
   ],
   "source": [
    "#quadratic polynomial function\n",
    "def f(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y.item())\n",
    "\n",
    "#gradient\n",
    "y.backward()\n",
    "print(\"df/dx = \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e544d34-d59c-4911-bff4-6a6e7ef55fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear polynomial function\n",
    "def lin_F(x):\n",
    "    W = torch.tensor([1.0], requires_grad=True)\n",
    "    b = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    assert x.shape[-1] == W.shape[0], \"\"\"\n",
    "    Invalid shape. (mxn)(nxp) = (mxp). Check shape. W.shape == 1\n",
    "    \"\"\"\n",
    "    return x@W + b, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297d7dc-1f28-4657-ad67-364d4c5f174f",
   "metadata": {},
   "source": [
    "## Understanding BackProg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd18de24-e063-40f5-b99a-e47c180bb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "df/dx (pT)=  -14.0\n",
      "df/dx (man_loss)=  -14.0\n",
      "df/dW (pT)=  -28.0\n",
      "df/db (pT)=  -14.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y_true = torch.tensor([10.0])\n",
    "\n",
    "#forward pass\n",
    "y_pred, W, b = lin_F(x)\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "#calculate the loss : mean squared error\n",
    "pT_loss = ((y_true - y_pred)**2).mean()\n",
    "man_loss = -2 * (y_true - y_pred).item()\n",
    "\n",
    "#gradient\n",
    "pT_loss.backward()\n",
    "print(\"df/dx (pT)= \", x.grad.item())\n",
    "print(\"df/dx (man_loss)= \", man_loss)\n",
    "\n",
    "# Gradient of loss w.r.t W2 and b2\n",
    "print(\"df/dW (pT)= \", W.grad.item())\n",
    "print(\"df/db (pT)= \", b.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30342ce6-ed1e-4150-b951-7ac4c5cb9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of output with respect to x: tensor([-14.])\n",
      "Gradient of output with respect to W: tensor([-28.])\n",
      "Gradient of output with respect to b: tensor([-14.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set up tensors with requires_grad=True\n",
    "W = torch.tensor([1.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Compute the linear operation\n",
    "y_pred = x@W + b\n",
    "y_true = torch.tensor([10.0])\n",
    "\n",
    "# Perform a dummy operation to compute gradients\n",
    "output = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "# Backward pass to compute the gradient of output with respect to W, b, and x\n",
    "output.backward()\n",
    "\n",
    "# Display the computed gradients\n",
    "print(\"Gradient of output with respect to x:\", x.grad)\n",
    "print(\"Gradient of output with respect to W:\", W.grad)\n",
    "print(\"Gradient of output with respect to b:\", b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5971f12-f201-49a2-a57e-ca7acd296ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf566c-5d02-4e57-9d69-b94625455875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed8bb9-75bb-4f45-9a62-7ad36ff6182c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120663dc-588f-4952-9942-631b3aa6a9e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Example Use Case\n",
    "\n",
    "### Dataset\n",
    "We use a synthetic dataset for demonstration purposes.\n",
    "\n",
    "### Preprocessing\n",
    "No preprocessing is required for this simple dataset.\n",
    "\n",
    "### Training the Model\n",
    "Training the Linear Neural Network using the synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306592d-b383-40cf-8a2d-f12acda59959",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30529c80-f987-49e4-87b3-72283f99a3e7",
   "metadata": {},
   "source": [
    "## Conclusion and Insights\n",
    "\n",
    "In this notebook, we have explored the fundamentals of Linear Neural Networks and implemented a simple model in PyTorch. Linear models are useful for understanding the basic principles of neural networks and serve as a foundation for more complex architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0ff78-a0b1-451f-95cb-b8b17a39c249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
