{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fe5df4-5903-4818-8d72-8d3571597c6d",
   "metadata": {},
   "source": [
    "# Linear Neural Networks\n",
    "\n",
    "In this notebook, we will explore the mathematical foundations and implementation of Linear Neural Networks. Linear models are the simplest form of neural networks and are primarily used for linear regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158649f-a43b-453e-85a2-a538c1b4f8e0",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Overview\n",
    "Linear Neural Networks are composed of layers where each neuron performs a linear transformation of the input.\n",
    "\n",
    "**Type of Function**: Linear\n",
    "\n",
    "**Nature**: Continuous\n",
    "\n",
    "**Behavior**: Linear neural networks are the simplest form of neural networks where the output is a linear combination of the input features. \n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "\n",
    "The output \\(y\\) is given by:\n",
    "\\[ y = XW + b1\\]\n",
    "where \\( X \\) is the input, \\( W \\) is the weight matrix, and \\( b \\) is the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e360f8-102b-4cac-956b-47ef5d687196",
   "metadata": {},
   "source": [
    "# Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b117ea2a-afc1-4b6f-8afb-07eb614baa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc1eb8-3f29-469d-a000-58bb680e14bd",
   "metadata": {},
   "source": [
    "### Mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73be90fe-5e43-4bdc-9afb-d72b6082622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) =  17.0\n",
      "df/dx =  8.0\n"
     ]
    }
   ],
   "source": [
    "#quadratic polynomial function\n",
    "def f(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y.item())\n",
    "\n",
    "#gradient\n",
    "y.backward()\n",
    "print(\"df/dx = \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e544d34-d59c-4911-bff4-6a6e7ef55fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear polynomial function\n",
    "def lin_F(x):\n",
    "    W = torch.tensor([1.0], requires_grad=True)\n",
    "    b = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    assert x.shape[-1] == W.shape[0], \"\"\"\n",
    "    Invalid shape. (mxn)(nxp) = (mxp). Check shape. W.shape == 1\n",
    "    \"\"\"\n",
    "    return x@W + b, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297d7dc-1f28-4657-ad67-364d4c5f174f",
   "metadata": {},
   "source": [
    "## Understanding BackProg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd18de24-e063-40f5-b99a-e47c180bb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "Loss =  49.0\n",
      "dL/y_pred (pT)=  -14.0\n",
      "dL/dW (pT)=  -28.0\n",
      "dL/db (pT)=  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch's .grad and .backward functions\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y_true = torch.tensor([10.0])\n",
    "\n",
    "#forward pass\n",
    "y_pred, W, b = lin_F(x)\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "#calculate the loss : mean squared error\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#gradient via PyTorch\n",
    "Loss.backward()\n",
    "print(\"Loss = \", Loss.item())\n",
    "print(\"dL/y_pred (pT)= \", x.grad.item())\n",
    "\n",
    "# Gradient of loss w.r.t W and b\n",
    "print(\"dL/dW (pT)= \", W.grad.item())\n",
    "print(\"dL/db (pT)= \", b.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30342ce6-ed1e-4150-b951-7ac4c5cb9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "dL/dy_pred =  -14.0\n",
      "dL/dW =  -28.0\n",
      "dL/db =  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Calculating gradients manually\n",
    "\n",
    "y_pred, W, b = lin_F(x)\n",
    "man_loss = ((y_true - y_pred)**2).mean()\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#differentiating Loss w.r.t y_pred (chain-rule)\n",
    "u = y_true - y_pred\n",
    "v = u**2\n",
    "\n",
    "du = -1 \n",
    "dv = 2*u\n",
    "dv_dypred = du*dv\n",
    "\n",
    "dL = dv_dypred.mean()\n",
    "print(\"dL/dy_pred = \", dL.item())\n",
    "\n",
    "#differentiating Loss w.r.t W (chain-rule)\n",
    "dW = dL * x\n",
    "dB = dL*1\n",
    "\n",
    "print(\"dL/dW = \", dW.item())\n",
    "print(\"dL/db = \", dB.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3a218-bb26-4518-97c2-ce9bbbc0c251",
   "metadata": {},
   "source": [
    "## Creating a simple two layers neural network with activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb1f3ce-2b00-468a-80d5-7b54ee3e0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119db2110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d421c5-6c2b-4ec0-aa97-d3b4e5101857",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(2, 3, requires_grad=True)  # Weights for layer 1\n",
    "b1 = torch.randn(2, requires_grad=True)     # Bias for layer 1\n",
    "\n",
    "W2 = torch.randn(1, 2, requires_grad=True)  # Weights for layer 2\n",
    "b2 = torch.randn(1, requires_grad=True)     # Bias for layer 2\n",
    "\n",
    "\n",
    "# Input\n",
    "X = torch.randn(1, 3)\n",
    "\n",
    "# Forward pass\n",
    "## Output of first layer\n",
    "z1 = X@W1.T + b1\n",
    "a1 = torch.relu(z1)          \n",
    "\n",
    "## Output of second layer\n",
    "z2 = a1@W2.T + b2 \n",
    "y_pred = z2                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840936fe-264d-43e6-a0a3-917faeeeac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y = torch.tensor([[1.0]])\n",
    "\n",
    "# MSE Loss\n",
    "loss = (y - y_pred).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9abd81d7-151b-4ecc-8f87-20c1d786dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient for W2:\n",
      " tensor([[-0.0000],\n",
      "        [-1.1464]], grad_fn=<MulBackward0>)\n",
      "Manual gradient for b2:\n",
      " tensor([[-3.4605]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gradients of loss with respect to output\n",
    "d_loss_output = -2 * (y - y_pred)\n",
    "\n",
    "# Gradients of output of layer 2 with respect to weights and biases\n",
    "d_output_W2 = a1.T\n",
    "d_output_b2 = 1\n",
    "\n",
    "# Gradient of loss with respect to W2 and b2\n",
    "grad_W2 = d_loss_output * d_output_W2\n",
    "grad_b2 = d_loss_output * d_output_b2\n",
    "\n",
    "print(\"Manual gradient for W2:\\n\", grad_W2)\n",
    "print(\"Manual gradient for b2:\\n\", grad_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6598eb05-bac2-4901-bfda-fb6ce55d26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd gradient for W2: tensor([[ 0.0000, -1.1464]])\n",
      "Autograd gradient for b2: tensor([-3.4605])\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss.backward()\n",
    "\n",
    "print(\"Autograd gradient for W2:\", W2.grad)\n",
    "print(\"Autograd gradient for b2:\", b2.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b18e15e5-50b8-4480-8555-dc9a3b0541f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient for W1:\n",
      " tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2541, -1.1957,  1.5362]], grad_fn=<MmBackward0>)\n",
      "Manual gradient for b1:\n",
      " tensor([[0.0000, 1.3958]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gradient of loss w.r.t. output of ReLU (a1)\n",
    "d_loss_a1 = d_loss_output@W2\n",
    "\n",
    "# Gradient of ReLU w.r.t. its input (z1)\n",
    "d_a1_z1 = (z1 > 0).float()  # Derivative of ReLU\n",
    "\n",
    "# Gradient of loss w.r.t. output of first layer (z1)\n",
    "d_loss_z1 = d_loss_a1 * d_a1_z1\n",
    "\n",
    "# Gradient of z1 w.r.t. W1 and b1\n",
    "d_z1_W1 = X.t()\n",
    "d_z1_b1 = 1\n",
    "\n",
    "# Gradient of loss w.r.t. W1 and b1\n",
    "grad_W1 = d_loss_z1.T@d_z1_W1.T\n",
    "grad_b1 = d_loss_z1 * d_z1_b1\n",
    "\n",
    "print(\"Manual gradient for W1:\\n\", grad_W1)\n",
    "print(\"Manual gradient for b1:\\n\", grad_b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88779ad5-9b7f-4c2a-9443-6923f3f12941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2541, -1.1957,  1.5362]]),\n",
       " tensor([0.0000, 1.3958]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.grad, b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33b66f8-c758-4d6e-94b5-dcbdad2d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters\n",
    "W1 = torch.randn(2, 3, requires_grad=False)\n",
    "b1 = torch.randn(2, requires_grad=False)\n",
    "W2 = torch.randn(1, 2, requires_grad=False)\n",
    "b2 = torch.randn(1, requires_grad=False)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd93d41-cb4f-476c-b3d4-550949383bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    # Forward pass through the network\n",
    "    z1 = torch.mm(X, W1.t()) + b1\n",
    "    a1 = torch.relu(z1)\n",
    "    z2 = torch.mm(a1, W2.t()) + b2\n",
    "    return z2, a1, z1\n",
    "\n",
    "def backward(X, a1, z1, z2, outputs, targets):\n",
    "    # Calculate loss (MSE)\n",
    "    loss = (outputs - targets).pow(2).mean()\n",
    "    \n",
    "    # Gradients of the loss w.r.t. output\n",
    "    d_loss_output = 2 * (outputs - targets) / outputs.size(0)\n",
    "    \n",
    "    # Gradients through the second layer\n",
    "    grad_W2 = torch.mm(d_loss_output.t(), a1).t()\n",
    "    grad_b2 = d_loss_output.sum(0)\n",
    "    \n",
    "    # Gradients through the first layer\n",
    "    d_loss_a1 = torch.mm(d_loss_output, W2)\n",
    "    d_a1_z1 = (z1 > 0).float()  # Derivative of ReLU\n",
    "    d_loss_z1 = d_loss_a1 * d_a1_z1\n",
    "    \n",
    "    grad_W1 = torch.mm(d_loss_z1.t(), X).t()\n",
    "    grad_b1 = d_loss_z1.sum(0)\n",
    "    \n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2, loss\n",
    "\n",
    "def update_params(grad_W1, grad_b1, grad_W2, grad_b2, lr):\n",
    "    # Update parameters using gradient descent\n",
    "    global W1, b1, W2, b2\n",
    "    W1 -= lr * grad_W1\n",
    "    b1 -= lr * grad_b1\n",
    "    W2 -= lr * grad_W2\n",
    "    b2 -= lr * grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4bd0fdb-3370-4840-9c44-1769d5fe8404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6164183616638184\n",
      "Epoch 10, Loss: 0.4384765326976776\n",
      "Epoch 20, Loss: 0.44407007098197937\n",
      "Epoch 30, Loss: 0.48292332887649536\n",
      "Epoch 40, Loss: 0.5079317688941956\n",
      "Epoch 50, Loss: 0.5129051208496094\n",
      "Epoch 60, Loss: 0.5133678913116455\n",
      "Epoch 70, Loss: 0.5344874858856201\n",
      "Epoch 80, Loss: 0.5579557418823242\n",
      "Epoch 90, Loss: 0.5685142874717712\n"
     ]
    }
   ],
   "source": [
    "# Dummy dataset\n",
    "X = torch.randn(100, 3)  # 100 samples, 3 features each\n",
    "targets = torch.randn(100, 1)  # 100 target values\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for i in range(X.size(0)):\n",
    "        inputs = X[i:i+1]\n",
    "        target = targets[i:i+1]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, a1, z1 = forward(inputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_W1, grad_b1, grad_W2, grad_b2, loss = backward(inputs, a1, z1, outputs, outputs, target)\n",
    "        \n",
    "        # Update parameters\n",
    "        update_params(grad_W1.T, grad_b1, grad_W2.T, grad_b2, lr)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120663dc-588f-4952-9942-631b3aa6a9e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Example Use Case\n",
    "\n",
    "### Dataset\n",
    "We use a synthetic dataset for demonstration purposes.\n",
    "\n",
    "### Preprocessing\n",
    "No preprocessing is required for this simple dataset.\n",
    "\n",
    "### Training the Model\n",
    "Training the Linear Neural Network using the synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306592d-b383-40cf-8a2d-f12acda59959",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cad38552-2460-4428-9b2a-65cb2d28c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.17363695800304413\n",
      "Epoch 10, Loss: 0.1932481825351715\n",
      "Epoch 20, Loss: 0.1880769431591034\n",
      "Epoch 30, Loss: 0.18764106929302216\n",
      "Epoch 40, Loss: 0.1884155124425888\n",
      "Epoch 50, Loss: 0.19187723100185394\n",
      "Epoch 60, Loss: 0.19805003702640533\n",
      "Epoch 70, Loss: 0.19886021316051483\n",
      "Epoch 80, Loss: 0.19674040377140045\n",
      "Epoch 90, Loss: 0.19662800431251526\n"
     ]
    }
   ],
   "source": [
    "# Dummy dataset\n",
    "X = torch.randn(100, 3)  # 100 samples, 3 features each\n",
    "targets = torch.randn(100, 1)  # 100 target values\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for i in range(X.size(0)):\n",
    "        inputs = X[i:i+1]\n",
    "        target = targets[i:i+1]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, a1, z1 = forward(inputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_W1, grad_b1, grad_W2, grad_b2, loss = backward(inputs, a1, z1, outputs, outputs, target)\n",
    "        \n",
    "        # Update parameters\n",
    "        update_params(grad_W1.T, grad_b1, grad_W2.T, grad_b2, lr)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30529c80-f987-49e4-87b3-72283f99a3e7",
   "metadata": {},
   "source": [
    "## Conclusion and Insights\n",
    "\n",
    "In this notebook, we have explored the fundamentals of Linear Neural Networks and implemented a simple model in PyTorch. Linear models are useful for understanding the basic principles of neural networks and serve as a foundation for more complex architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29204f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
