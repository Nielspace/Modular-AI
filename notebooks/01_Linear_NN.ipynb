{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fe5df4-5903-4818-8d72-8d3571597c6d",
   "metadata": {},
   "source": [
    "# Linear Neural Networks\n",
    "\n",
    "In this notebook, we will explore the mathematical foundations and implementation of Linear Neural Networks. Linear models are the simplest form of neural networks and are primarily used for linear regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158649f-a43b-453e-85a2-a538c1b4f8e0",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Overview\n",
    "Linear Neural Networks are composed of layers where each neuron performs a linear transformation of the input.\n",
    "\n",
    "**Type of Function**: Linear\n",
    "\n",
    "**Nature**: Continuous\n",
    "\n",
    "**Behavior**: Linear neural networks are the simplest form of neural networks where the output is a linear combination of the input features. \n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "\n",
    "The output \\(y\\) is given by:\n",
    "\\[ y = XW + b1\\]\n",
    "where \\( X \\) is the input, \\( W \\) is the weight matrix, and \\( b \\) is the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e360f8-102b-4cac-956b-47ef5d687196",
   "metadata": {},
   "source": [
    "# Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b117ea2a-afc1-4b6f-8afb-07eb614baa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc1eb8-3f29-469d-a000-58bb680e14bd",
   "metadata": {},
   "source": [
    "### Mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73be90fe-5e43-4bdc-9afb-d72b6082622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) =  17.0\n",
      "df/dx =  8.0\n"
     ]
    }
   ],
   "source": [
    "#quadratic polynomial function\n",
    "def f(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y.item())\n",
    "\n",
    "#gradient\n",
    "y.backward()\n",
    "print(\"df/dx = \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e544d34-d59c-4911-bff4-6a6e7ef55fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear polynomial function\n",
    "def lin_F(x):\n",
    "    W = torch.tensor([1.0], requires_grad=True)\n",
    "    b = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    assert x.shape[-1] == W.shape[0], \"\"\"\n",
    "    Invalid shape. (mxn)(nxp) = (mxp). Check shape. W.shape == 1\n",
    "    \"\"\"\n",
    "    return x@W + b, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297d7dc-1f28-4657-ad67-364d4c5f174f",
   "metadata": {},
   "source": [
    "## Understanding BackProg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd18de24-e063-40f5-b99a-e47c180bb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "Loss =  49.0\n",
      "dL/y_pred (pT)=  -14.0\n",
      "dL/dW (pT)=  -28.0\n",
      "dL/db (pT)=  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch's .grad and .backward functions\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y_true = torch.tensor([10.0])\n",
    "\n",
    "#forward pass\n",
    "y_pred, W, b = lin_F(x)\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "#calculate the loss : mean squared error\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#gradient via PyTorch\n",
    "Loss.backward()\n",
    "print(\"Loss = \", Loss.item())\n",
    "print(\"dL/y_pred (pT)= \", x.grad.item())\n",
    "\n",
    "# Gradient of loss w.r.t W and b\n",
    "print(\"dL/dW (pT)= \", W.grad.item())\n",
    "print(\"dL/db (pT)= \", b.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30342ce6-ed1e-4150-b951-7ac4c5cb9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "dL/dy_pred =  -14.0\n",
      "dL/dW =  -28.0\n",
      "dL/db =  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Calculating gradients manually\n",
    "\n",
    "y_pred, W, b = lin_F(x)\n",
    "man_loss = ((y_true - y_pred)**2).mean()\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#differentiating Loss w.r.t y_pred (chain-rule)\n",
    "u = y_true - y_pred\n",
    "v = u**2\n",
    "\n",
    "du = -1 \n",
    "dv = 2*u\n",
    "dv_dypred = du*dv\n",
    "\n",
    "dL = dv_dypred.mean()\n",
    "print(\"dL/dy_pred = \", dL.item())\n",
    "\n",
    "#differentiating Loss w.r.t W (chain-rule)\n",
    "dW = dL * x\n",
    "dB = dL*1\n",
    "\n",
    "print(\"dL/dW = \", dW.item())\n",
    "print(\"dL/db = \", dB.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3a218-bb26-4518-97c2-ce9bbbc0c251",
   "metadata": {},
   "source": [
    "## Creating a simple two layers neural network with activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bb1f3ce-2b00-468a-80d5-7b54ee3e0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116b12e90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d421c5-6c2b-4ec0-aa97-d3b4e5101857",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(2, 3, requires_grad=True)  # Weights for layer 1\n",
    "b1 = torch.randn(2, requires_grad=True)     # Bias for layer 1\n",
    "\n",
    "W2 = torch.randn(1, 2, requires_grad=True)  # Weights for layer 2\n",
    "b2 = torch.randn(1, requires_grad=True)     # Bias for layer 2\n",
    "\n",
    "\n",
    "# Input\n",
    "X = torch.randn(1, 3)\n",
    "\n",
    "# Forward pass\n",
    "## Output of first layer\n",
    "z1 = X@W1.T + b1\n",
    "a1 = torch.relu(z1)          \n",
    "\n",
    "## Output of second layer\n",
    "z2 = a1@W2.T + b2 \n",
    "y_pred = z2                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840936fe-264d-43e6-a0a3-917faeeeac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y = torch.tensor([[1.0]])\n",
    "\n",
    "# MSE Loss\n",
    "loss = (y - y_pred).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9abd81d7-151b-4ecc-8f87-20c1d786dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient for W2:\n",
      " tensor([[ 0.0000],\n",
      "        [21.3885]], grad_fn=<MulBackward0>)\n",
      "Manual gradient for b2:\n",
      " tensor([[5.0055]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gradients of loss with respect to output\n",
    "d_loss_output = -2 * (y - y_pred)\n",
    "\n",
    "# Gradients of output of layer 2 with respect to weights and biases\n",
    "d_output_W2 = a1.T\n",
    "d_output_b2 = 1\n",
    "\n",
    "# Gradient of loss with respect to W2 and b2\n",
    "grad_W2 = d_loss_output * d_output_W2\n",
    "grad_b2 = d_loss_output * d_output_b2\n",
    "\n",
    "print(\"Manual gradient for W2:\\n\", grad_W2)\n",
    "print(\"Manual gradient for b2:\\n\", grad_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6598eb05-bac2-4901-bfda-fb6ce55d26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd gradient for W2: tensor([[ 0.0000, 21.3885]])\n",
      "Autograd gradient for b2: tensor([5.0055])\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss.backward()\n",
    "\n",
    "print(\"Autograd gradient for W2:\", W2.grad)\n",
    "print(\"Autograd gradient for b2:\", b2.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b18e15e5-50b8-4480-8555-dc9a3b0541f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient for W1:\n",
      " tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 4.9017, -5.0050, -3.0683]], grad_fn=<MmBackward0>)\n",
      "Manual gradient for b1:\n",
      " tensor([[0.0000, 3.8785]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gradient of loss w.r.t. output of ReLU (a1)\n",
    "d_loss_a1 = d_loss_output@W2\n",
    "\n",
    "# Gradient of ReLU w.r.t. its input (z1)\n",
    "d_a1_z1 = (z1 > 0).float()  # Derivative of ReLU\n",
    "\n",
    "# Gradient of loss w.r.t. output of first layer (z1)\n",
    "d_loss_z1 = d_loss_a1 * d_a1_z1\n",
    "\n",
    "# Gradient of z1 w.r.t. W1 and b1\n",
    "d_z1_W1 = X.t()\n",
    "d_z1_b1 = 1\n",
    "\n",
    "# Gradient of loss w.r.t. W1 and b1\n",
    "grad_W1 = d_loss_z1.T@d_z1_W1.T\n",
    "grad_b1 = d_loss_z1 * d_z1_b1\n",
    "\n",
    "print(\"Manual gradient for W1:\\n\", grad_W1)\n",
    "print(\"Manual gradient for b1:\\n\", grad_b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88779ad5-9b7f-4c2a-9443-6923f3f12941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 4.9017, -5.0050, -3.0683]]),\n",
       " tensor([0.0000, 3.8785]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.grad, b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e33b66f8-c758-4d6e-94b5-dcbdad2d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Parameters\n",
    "W1 = torch.randn(2, 3, requires_grad=False)\n",
    "b1 = torch.randn(2, requires_grad=False)\n",
    "W2 = torch.randn(1, 2, requires_grad=False)\n",
    "b2 = torch.randn(1, requires_grad=False)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecd93d41-cb4f-476c-b3d4-550949383bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    # Forward pass through the network\n",
    "    z1 = torch.mm(X, W1.t()) + b1\n",
    "    a1 = torch.relu(z1)\n",
    "    z2 = torch.mm(a1, W2.t()) + b2\n",
    "    return z2, a1, z1\n",
    "\n",
    "def backward(X, a1, z1, z2, outputs, targets):\n",
    "    # Calculate loss (MSE)\n",
    "    loss = (outputs - targets).pow(2).mean()\n",
    "    \n",
    "    # Gradients of the loss w.r.t. output\n",
    "    d_loss_output = 2 * (outputs - targets) / outputs.size(0)\n",
    "    \n",
    "    # Gradients through the second layer\n",
    "    grad_W2 = torch.mm(d_loss_output.t(), a1).t()\n",
    "    grad_b2 = d_loss_output.sum(0)\n",
    "    \n",
    "    # Gradients through the first layer\n",
    "    d_loss_a1 = torch.mm(d_loss_output, W2)\n",
    "    d_a1_z1 = (z1 > 0).float()  # Derivative of ReLU\n",
    "    d_loss_z1 = d_loss_a1 * d_a1_z1\n",
    "    \n",
    "    grad_W1 = torch.mm(d_loss_z1.t(), X).t()\n",
    "    grad_b1 = d_loss_z1.sum(0)\n",
    "    \n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2, loss\n",
    "\n",
    "def update_params(grad_W1, grad_b1, grad_W2, grad_b2, lr):\n",
    "    # Update parameters using gradient descent\n",
    "    global W1, b1, W2, b2\n",
    "    W1 -= lr * grad_W1\n",
    "    b1 -= lr * grad_b1\n",
    "    W2 -= lr * grad_W2\n",
    "    b2 -= lr * grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4bd0fdb-3370-4840-9c44-1769d5fe8404",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     grad_W1, grad_b1, grad_W2, grad_b2, loss \u001b[38;5;241m=\u001b[39m backward(inputs, a1, z1, outputs, outputs, target)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_W1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_b1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_W2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[30], line 32\u001b[0m, in \u001b[0;36mupdate_params\u001b[0;34m(grad_W1, grad_b1, grad_W2, grad_b2, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m(grad_W1, grad_b1, grad_W2, grad_b2, lr):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Update parameters using gradient descent\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m W1, b1, W2, b2\n\u001b[0;32m---> 32\u001b[0m     W1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad_W1\n\u001b[1;32m     33\u001b[0m     b1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad_b1\n\u001b[1;32m     34\u001b[0m     W2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad_W2\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Dummy dataset\n",
    "X = torch.randn(100, 3)  # 100 samples, 3 features each\n",
    "targets = torch.randn(100, 1)  # 100 target values\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for i in range(X.size(0)):\n",
    "        inputs = X[i:i+1]\n",
    "        target = targets[i:i+1]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, a1, z1 = forward(inputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_W1, grad_b1, grad_W2, grad_b2, loss = backward(inputs, a1, z1, outputs, outputs, target)\n",
    "        \n",
    "        # Update parameters\n",
    "        update_params(grad_W1, grad_b1, grad_W2, grad_b2, lr)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120663dc-588f-4952-9942-631b3aa6a9e7",
   "metadata": {},
   "source": [
    "## Example Use Case\n",
    "\n",
    "### Dataset\n",
    "We use a synthetic dataset for demonstration purposes.\n",
    "\n",
    "### Preprocessing\n",
    "No preprocessing is required for this simple dataset.\n",
    "\n",
    "### Training the Model\n",
    "Training the Linear Neural Network using the synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306592d-b383-40cf-8a2d-f12acda59959",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30529c80-f987-49e4-87b3-72283f99a3e7",
   "metadata": {},
   "source": [
    "## Conclusion and Insights\n",
    "\n",
    "In this notebook, we have explored the fundamentals of Linear Neural Networks and implemented a simple model in PyTorch. Linear models are useful for understanding the basic principles of neural networks and serve as a foundation for more complex architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaab874-53a7-4944-bd51-230fb49b6bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
