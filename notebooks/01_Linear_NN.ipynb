{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fe5df4-5903-4818-8d72-8d3571597c6d",
   "metadata": {},
   "source": [
    "# Linear Neural Networks\n",
    "\n",
    "In this notebook, we will explore the mathematical foundations and implementation of Linear Neural Networks. Linear models are the simplest form of neural networks and are primarily used for linear regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158649f-a43b-453e-85a2-a538c1b4f8e0",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Overview\n",
    "Linear Neural Networks are composed of layers where each neuron performs a linear transformation of the input.\n",
    "\n",
    "**Type of Function**: Linear\n",
    "\n",
    "**Nature**: Continuous\n",
    "\n",
    "**Behavior**: Linear neural networks are the simplest form of neural networks where the output is a linear combination of the input features. \n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "\n",
    "The output \\(y\\) is given by:\n",
    "\\[ y = XW + b1\\]\n",
    "where \\( X \\) is the input, \\( W \\) is the weight matrix, and \\( b \\) is the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e360f8-102b-4cac-956b-47ef5d687196",
   "metadata": {},
   "source": [
    "# Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b117ea2a-afc1-4b6f-8afb-07eb614baa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc1eb8-3f29-469d-a000-58bb680e14bd",
   "metadata": {},
   "source": [
    "### Mathematical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73be90fe-5e43-4bdc-9afb-d72b6082622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) =  17.0\n",
      "df/dx =  8.0\n"
     ]
    }
   ],
   "source": [
    "#quadratic polynomial function\n",
    "def f(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = f(x)\n",
    "print(\"f(x) = \", y.item())\n",
    "\n",
    "#gradient\n",
    "y.backward()\n",
    "print(\"df/dx = \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e544d34-d59c-4911-bff4-6a6e7ef55fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear polynomial function\n",
    "def lin_F(x):\n",
    "    W = torch.tensor([1.0], requires_grad=True)\n",
    "    b = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    assert x.shape[-1] == W.shape[0], \"\"\"\n",
    "    Invalid shape. (mxn)(nxp) = (mxp). Check shape. W.shape == 1\n",
    "    \"\"\"\n",
    "    return x@W + b, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297d7dc-1f28-4657-ad67-364d4c5f174f",
   "metadata": {},
   "source": [
    "## Understanding BackProg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd18de24-e063-40f5-b99a-e47c180bb0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "Loss =  49.0\n",
      "dL/y_pred (pT)=  -14.0\n",
      "dL/dW (pT)=  -28.0\n",
      "dL/db (pT)=  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Using PyTorch's .grad and .backward functions\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y_true = torch.tensor([10.0])\n",
    "\n",
    "#forward pass\n",
    "y_pred, W, b = lin_F(x)\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "#calculate the loss : mean squared error\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#gradient via PyTorch\n",
    "Loss.backward()\n",
    "print(\"Loss = \", Loss.item())\n",
    "print(\"dL/y_pred (pT)= \", x.grad.item())\n",
    "\n",
    "# Gradient of loss w.r.t W and b\n",
    "print(\"dL/dW (pT)= \", W.grad.item())\n",
    "print(\"dL/db (pT)= \", b.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30342ce6-ed1e-4150-b951-7ac4c5cb9858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3.0\n",
      "dL/dy_pred =  -14.0\n",
      "dL/dW =  -28.0\n",
      "dL/db =  -14.0\n"
     ]
    }
   ],
   "source": [
    "# Calculating gradients manually\n",
    "\n",
    "y_pred, W, b = lin_F(x)\n",
    "man_loss = ((y_true - y_pred)**2).mean()\n",
    "print(\"y =\", y_pred.item())\n",
    "\n",
    "Loss = ((y_true - y_pred)**2).mean()\n",
    "\n",
    "#differentiating Loss w.r.t y_pred (chain-rule)\n",
    "u = y_true - y_pred\n",
    "v = u**2\n",
    "\n",
    "du = -1 \n",
    "dv = 2*u\n",
    "dv_dypred = du*dv\n",
    "\n",
    "dL = dv_dypred.mean()\n",
    "print(\"dL/dy_pred = \", dL.item())\n",
    "\n",
    "#differentiating Loss w.r.t W (chain-rule)\n",
    "dW = dL * x\n",
    "dB = dL*1\n",
    "\n",
    "print(\"dL/dW = \", dW.item())\n",
    "print(\"dL/db = \", dB.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3a218-bb26-4518-97c2-ce9bbbc0c251",
   "metadata": {},
   "source": [
    "## Creating a simple two layers neural network with activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67161d2-9843-40fb-b7f2-5196c6bb227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1., 1., 1., 1.], requires_grad = True)\n",
    "y = torch.tensor([12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f9e5b9-8f0d-42b9-8503-9b780161afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  9.0\n",
      "dL/y_pred (pT)=  tensor([[-1.5000],\n",
      "        [-1.5000],\n",
      "        [-1.5000],\n",
      "        [-1.5000]])\n",
      "dL/dW1 (pT)=  tensor([[-6.],\n",
      "        [-6.],\n",
      "        [-6.],\n",
      "        [-6.]])\n",
      "dL/db1 (pT)=  tensor([-24.])\n",
      "dL/dW2 (pT)=  tensor([[-12., -12., -12., -12.]])\n",
      "dL/db2 (pT)=  tensor([-6.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining a neural network\n",
    "\n",
    "\n",
    "W1 = torch.tensor([[1.], [1.], [1.], [1.]], requires_grad=True)\n",
    "b1 = torch.tensor([1.], requires_grad=True)\n",
    "W2 = torch.tensor([[1., 1., 1., 1.]], requires_grad=True)\n",
    "b2 = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "hidden = torch.nn.functional.linear(X.view(-1,1), W1, b1)\n",
    "y_pred = torch.nn.functional.linear(hidden, W2, b2)\n",
    "y_pred.retain_grad()\n",
    "\n",
    "Loss = ((y - y_pred)**2).mean()\n",
    "print(\"Loss = \", Loss.item()) \n",
    "\n",
    "Loss.backward()\n",
    "\n",
    "print(\"dL/y_pred (pT)= \", y_pred.grad)\n",
    "\n",
    "# Gradient of loss w.r.t W and b\n",
    "print(\"dL/dW1 (pT)= \", W1.grad)\n",
    "print(\"dL/db1 (pT)= \", b1.grad)\n",
    "\n",
    "print(\"dL/dW2 (pT)= \", W2.grad)\n",
    "print(\"dL/db2 (pT)= \", b2.grad)\n",
    "\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7ed363-bc31-4f9b-8908-9346351f9d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  9.0\n",
      "dL/dy_pred =  -6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss = \", Loss.item()) \n",
    "\n",
    "u = y - y_pred\n",
    "v = u**2\n",
    "\n",
    "du = -1 \n",
    "dv = 2*u\n",
    "dv_dypred = du*dv\n",
    "\n",
    "dL = dv_dypred.mean()\n",
    "print(\"dL/dy_pred = \", dL.item())\n",
    "\n",
    "# #differentiating Loss w.r.t W (chain-rule)\n",
    "# dW = dL * x\n",
    "# dB = dL*1\n",
    "\n",
    "# print(\"dL/dW = \", dW.item())\n",
    "# print(\"dL/db = \", dB.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120663dc-588f-4952-9942-631b3aa6a9e7",
   "metadata": {},
   "source": [
    "## Example Use Case\n",
    "\n",
    "### Dataset\n",
    "We use a synthetic dataset for demonstration purposes.\n",
    "\n",
    "### Preprocessing\n",
    "No preprocessing is required for this simple dataset.\n",
    "\n",
    "### Training the Model\n",
    "Training the Linear Neural Network using the synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306592d-b383-40cf-8a2d-f12acda59959",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30529c80-f987-49e4-87b3-72283f99a3e7",
   "metadata": {},
   "source": [
    "## Conclusion and Insights\n",
    "\n",
    "In this notebook, we have explored the fundamentals of Linear Neural Networks and implemented a simple model in PyTorch. Linear models are useful for understanding the basic principles of neural networks and serve as a foundation for more complex architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17b0ff78-a0b1-451f-95cb-b8b17a39c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to X: tensor([-1.5000, -1.5000, -1.5000, -1.5000])\n",
      "Gradient with respect to W: tensor([[-1.5000],\n",
      "        [-1.5000],\n",
      "        [-1.5000],\n",
      "        [-1.5000]])\n",
      "Gradient with respect to b: tensor([-6.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([1., 1., 1., 1.], requires_grad=True)\n",
    "# Weights and bias\n",
    "W = torch.tensor([[1.], [1.], [1.], [1.]], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "# True output for MSE calculation\n",
    "y_true = torch.tensor([5.], requires_grad=False)  # no need for gradients\n",
    "\n",
    "# Perform the linear transformation using functional API\n",
    "y_pred = F.linear(X.view(-1, 1), W, b)  # Reshape X to match expected dimensions\n",
    "\n",
    "# Define the loss function (MSE)\n",
    "loss = (y_true - y_pred).pow(2).mean()\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()\n",
    "\n",
    "# Output the gradients\n",
    "print(\"Gradient with respect to X:\", X.grad)\n",
    "print(\"Gradient with respect to W:\", W.grad)\n",
    "print(\"Gradient with respect to b:\", b.grad)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768b125-75ae-47bc-a8f0-86a201d91e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd81d7-151b-4ecc-8f87-20c1d786dac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
