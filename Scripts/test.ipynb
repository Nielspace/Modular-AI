{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import data utils"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[11736,   377, 20106,  ..., 28836,   425,  3912],\n","        [   64,  7751,   291,  ...,   291,  5768,  3264]]) tensor([[  377, 20106,   292,  ...,   425,  3912,   543],\n","        [ 7751,   291,  1741,  ...,  5768,  3264,   777]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]])\n"]}],"source":["import pandas as pd\n","from tokenise import TextDataset, pad_sequences\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","df = pd.read_parquet(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")\n","articles = df.iloc[:, 1]\n","\n","dataset = TextDataset(articles, model=\"gpt2\", seq_length=512)\n","\n","# Create the dataloader\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_sequences)\n","\n","for data in dataloader:\n","    x, y, att = data['input_ids'], data['targets'], data['attention_mask']\n","    print(x,y,att)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Import models"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from dataclasses import dataclass\n","\n","@dataclass\n","class GPTConfig:\n","    vocab_size = dataset.vocab_size\n","    block_size = 1024\n","    n_layer = 6\n","    n_head = 8\n","    n_embd = 512\n","    dropout = 0.1\n","    bias = False\n","\n","config = GPTConfig()"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer blocks"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from customGPT import Block, GPT"]},{"cell_type":"markdown","metadata":{},"source":["## Attention"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torch \n","from torch import nn\n","import torch.nn.functional as F \n","\n","import math"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 44.63M\n"]},{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(50257, 512)\n","    (wpe): Embedding(1024, 512)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): CausalSelfAttention(\n","          (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n","          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","          (3): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from attention import CausalSelfAttention\n","\n","C_GPT = GPT(CausalSelfAttention, config)\n","C_GPT"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 44.65M\n"]},{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(50257, 512)\n","    (wpe): Embedding(1024, 512)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): FlashAttention(\n","          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","          (proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","          (3): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from attention import FlashAttention\n","F_GPT = GPT(FlashAttention, config)\n","F_GPT"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 44.65M\n"]},{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(50257, 512)\n","    (wpe): Embedding(1024, 512)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): SparseAttention(\n","          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","          (proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","          (3): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=50257, bias=False)\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from attention import SparseAttention\n","S_GPT = GPT(SparseAttention, config)\n","S_GPT"]},{"cell_type":"markdown","metadata":{},"source":["# 5 epoch test"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#training dependencies\n","import numpy as np\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.cuda.amp import GradScaler\n","from contextlib import nullcontext\n","import torch.distributed as dist\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","import os"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens per iteration will be: 40,960\n"]}],"source":["# Default config values designed to train a GPT-2 (124M) on OpenWebText\n","# I/O\n","out_dir = 'out'\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False\n","always_save_checkpoint = True\n","init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n","\n","# Data\n","# data_path = 'data/openwebtext.txt'  # Path to your text file\n","gradient_accumulation_steps = 5 * 8\n","batch_size = 2\n","block_size = 512\n","\n","# Model\n","n_layer = 6  # Reduce the number of layers\n","n_head = 8   # Reduce the number of attention heads\n","n_embd = 512 # Reduce the embedding size\n","dropout = 0.0\n","bias = False\n","\n","# AdamW optimizer\n","learning_rate = 6e-4\n","max_iters = 5\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0\n","\n","# Learning rate decay settings\n","decay_lr = True\n","warmup_iters = 200\n","lr_decay_iters = 1000\n","min_lr = 6e-5\n","\n","# DDP settings\n","backend = 'gloo'\n","\n","# System\n","device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n","dtype = 'float32'  # MPS currently supports only float32\n","compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n","# -----------------------------------------------------------------------------\n","\n","# Collect configuration keys\n","config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","\n","# Various initializations and derived attributes\n","ddp = int(os.environ.get('RANK', -1)) != -1\n","if ddp:\n","    init_process_group(backend=backend)\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n","    torch.mps.set_device(device)\n","    master_process = ddp_rank == 0\n","    seed_offset = ddp_rank\n","    assert gradient_accumulation_steps % ddp_world_size == 0\n","    gradient_accumulation_steps //= ddp_world_size\n","else:\n","    master_process = True\n","    seed_offset = 0\n","    ddp_world_size = 1\n","\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'mps' if 'mps' in device else 'cpu'\n","ptdtype = torch.float32  # Currently, MPS supports only float32\n","ctx = nullcontext()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing a new model from scratch\n","Number of parameters: 19.18M\n"]},{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(512, 512)\n","    (wpe): Embedding(512, 512)\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): SparseAttention(\n","          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","          (proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","          (3): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=512, bias=False)\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#configuration\n","class GPTConfig:\n","    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias=True):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.n_embd = n_embd\n","        self.dropout = dropout\n","        self.bias = bias\n","        \n","# Initialize iteration number and best validation loss\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","# Model initialization\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout)\n","\n","if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    gptconf = GPTConfig(**model_args)\n","\n","    #select Attentions\n","    model = GPT(SparseAttention, gptconf)\n","elif init_from == 'resume':\n","    print(f\"Resuming training from {out_dir}\")\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k, v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num']\n","    best_val_loss = checkpoint['best_val_loss']\n","elif init_from.startswith('gpt2'):\n","    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n","    override_args = dict(dropout=dropout)\n","    model = GPT.from_pretrained(init_from, override_args)\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = getattr(model.config, k)\n","\n","if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize a GradScaler\n","scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n","\n","# Configure the optimizer\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","if init_from == 'resume':\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","checkpoint = None  # Free up memory\n","\n","# Wrap model into DDP container if needed\n","if ddp:\n","    print(f\"Starting parallel process with rank {ddp_rank}\")\n","    model = DDP(model, device_ids=[ddp_local_rank])\n","\n","\n","# Function to estimate loss over splits\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n","            for k in range(eval_iters):\n","                batch = next(iter(dataloader))\n","                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","                with ctx:\n","                    logits, loss = model(X, Y)\n","                losses[k] = loss.item()\n","                pbar.update(1)\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# Learning rate decay scheduler\n","def get_lr(it):\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    if it > lr_decay_iters:\n","        return min_lr\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating train: 100%|██████████| 200/200 [00:21<00:00,  9.28it/s]\n","Evaluating val: 100%|██████████| 200/200 [00:19<00:00, 10.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["step 0: train loss 2.9166, val loss 2.9313\n"]},{"name":"stderr","output_type":"stream","text":["Training step 0: 100%|██████████| 40/40 [00:11<00:00,  3.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 0: loss 2.9253, time 52281.32ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 1: 100%|██████████| 40/40 [00:11<00:00,  3.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 1: loss 3.0903, time 11924.31ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 2: 100%|██████████| 40/40 [00:12<00:00,  3.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 2: loss 2.5330, time 13175.06ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 3: 100%|██████████| 40/40 [00:11<00:00,  3.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 3: loss 2.2273, time 12327.11ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 4: 100%|██████████| 40/40 [00:10<00:00,  3.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 4: loss 3.6481, time 11076.78ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 5: 100%|██████████| 40/40 [00:11<00:00,  3.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 5: loss 2.1135, time 11989.06ms, mfu 0.15%\n"]}],"source":["import time\n","from tqdm import tqdm\n","\n","\n","# wandb.init(\n","#     # set the wandb project where this run will be logged\n","#     project=project_name, name= run_name, config=config)\n","\n","raw_model = model.module if ddp else model\n","local_iter_num = 0\n","running_mfu = -1.0\n","t0 = time.time()\n","\n","while iter_num <= max_iters:\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","        # if wandb_log:\n","        #     wandb.log({\n","        #         \"iter\": iter_num,\n","        #         \"train_loss\": losses['train'],\n","        #         \"val_loss\": losses['val'],\n","        #         \"lr\": lr,\n","        #         \"mfu\": running_mfu * 100,  # convert to percentage\n","        #     })\n","\n","\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n","        total_train_loss = 0.0  # Initialize total training loss\n","        for micro_step in range(gradient_accumulation_steps):\n","            if ddp:\n","                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n","            batch = next(iter(dataloader))\n","            X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","                loss = loss / gradient_accumulation_steps\n","            scaler.scale(loss).backward()\n","            pbar.update(1)\n","\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5:\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n","\n","        # if wandb_log:\n","        #     wandb.log({\n","        #         \"iter\": iter_num,\n","        #         \"train_loss\": total_train_loss,\n","        #         \"lr\": lr,\n","        #         \"mfu\": running_mfu * 100,  # convert to percentage\n","        #     })\n","\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","if ddp:\n","    destroy_process_group()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}
