{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Loaded\n","Tokens per iteration will be: 40,960\n","Initializing a new model from scratch\n","Number of parameters: 19.18M\n","Attention Selected\n","Training Started\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating train: 100%|██████████| 200/200 [00:17<00:00, 11.17it/s]\n","Evaluating val: 100%|██████████| 200/200 [00:18<00:00, 11.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["step 0: train loss 2.9258, val loss 2.9747\n"]},{"name":"stderr","output_type":"stream","text":["Training step 0: 100%|██████████| 40/40 [00:10<00:00,  3.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 0: loss 3.8065, time 46633.08ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 1: 100%|██████████| 40/40 [00:10<00:00,  3.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 1: loss 4.0380, time 10759.00ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 2: 100%|██████████| 40/40 [00:10<00:00,  3.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 2: loss 2.5838, time 10878.55ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 3: 100%|██████████| 40/40 [00:10<00:00,  3.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 3: loss 2.4303, time 11147.20ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 4: 100%|██████████| 40/40 [00:11<00:00,  3.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 4: loss 2.4450, time 11823.15ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 5: 100%|██████████| 40/40 [00:10<00:00,  3.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 5: loss 1.9745, time 11325.09ms, mfu 0.16%\n"]}],"source":["import train"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH]\n","                             [--batch_size BATCH_SIZE]\n","                             [--block_size BLOCK_SIZE] [--n_layer N_LAYER]\n","                             [--n_head N_HEAD] [--n_embd N_EMBD]\n","                             [--dropout DROPOUT]\n","                             [--learning_rate LEARNING_RATE]\n","                             [--max_iters MAX_ITERS] [--grad_clip GRAD_CLIP]\n","                             [--eval_interval EVAL_INTERVAL]\n","                             [--eval_iters EVAL_ITERS]\n","                             [--log_interval LOG_INTERVAL]\n","                             [--init_from INIT_FROM] [--out_dir OUT_DIR]\n","                             [--backend BACKEND] [--device DEVICE]\n","ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/nielspace/Library/Jupyter/runtime/kernel-v2-83046Y7ecPvgzXVVZ.json\n"]},{"ename":"SystemExit","evalue":"2","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/Caskroom/miniforge/base/envs/pytorchenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}],"source":["# Imports\n","import os\n","import math\n","import time\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import GradScaler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from contextlib import nullcontext\n","import torch.distributed as dist\n","from tqdm import tqdm\n","import wandb\n","\n","from tokenise import TextDataset, pad_sequences\n","from attention import SparseAttention\n","from customGPT import Block, GPT\n","\n","# Constants\n","OUT_DIR = 'out'\n","EVAL_INTERVAL = 2000\n","LOG_INTERVAL = 1\n","EVAL_ITERS = 200\n","EVAL_ONLY = False\n","ALWAYS_SAVE_CHECKPOINT = True\n","INIT_FROM = 'scratch'\n","DATA_PATH = 'wiki_medical_terms'\n","GRAD_ACCUMULATION_STEPS = 5 * 8\n","BATCH_SIZE = 2\n","BLOCK_SIZE = 512\n","LOCAL_ATTN_CTX = 32\n","ATTN_MODE = \"local\"\n","N_LAYER = 6\n","N_HEAD = 8\n","N_EMBD = 512\n","DROPOUT = 0.0\n","BIAS = False\n","LEARNING_RATE = 6e-4\n","MAX_ITERS = 5\n","WEIGHT_DECAY = 1e-1\n","BETA1 = 0.9\n","BETA2 = 0.95\n","GRAD_CLIP = 1.0\n","DECAY_LR = True\n","WARMUP_ITERS = 200\n","LR_DECAY_ITERS = 1000\n","MIN_LR = 6e-5\n","BACKEND = 'gloo'\n","PROJECT_NAME = \"Attention Benchmark\"\n","RUN_NAME = 'Sparse Attention-mps'\n","\n","# System configuration\n","DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'\n","DTYPE = 'float32'\n","COMPILE = False\n","\n","# Functions and Classes\n","def load_data(data_path):\n","    df = pd.read_parquet(data_path)\n","    articles = df.iloc[:, 1]\n","    dataset = TextDataset(articles, model=\"gpt2\", seq_length=BLOCK_SIZE)\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_sequences)\n","    return dataloader\n","\n","def init_ddp():\n","    ddp = int(os.environ.get('RANK', -1)) != -1\n","    if ddp:\n","        dist.init_process_group(backend=BACKEND)\n","        ddp_rank = int(os.environ['RANK'])\n","        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","        ddp_world_size = int(os.environ['WORLD_SIZE'])\n","        device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n","        torch.mps.set_device(device)\n","        master_process = ddp_rank == 0\n","        seed_offset = ddp_rank\n","        assert GRAD_ACCUMULATION_STEPS % ddp_world_size == 0\n","        grad_accumulation_steps = GRAD_ACCUMULATION_STEPS // ddp_world_size\n","    else:\n","        master_process = True\n","        seed_offset = 0\n","        ddp_world_size = 1\n","        grad_accumulation_steps = GRAD_ACCUMULATION_STEPS\n","    return ddp, master_process, device, seed_offset, ddp_world_size, grad_accumulation_steps\n","\n","def configure_optimizers(model, weight_decay, learning_rate, betas, device_type):\n","    return model.configure_optimizers(weight_decay, learning_rate, betas, device_type)\n","\n","def estimate_loss(model, dataloader, eval_iters, device, ctx):\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n","            for k in range(eval_iters):\n","                batch = next(iter(dataloader))\n","                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","                with ctx:\n","                    logits, loss = model(X, Y)\n","                losses[k] = loss.item()\n","                pbar.update(1)\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","def get_lr(it, learning_rate, warmup_iters, lr_decay_iters, min_lr):\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    if it > lr_decay_iters:\n","        return min_lr\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)\n","\n","class GPTConfig:\n","    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, local_attn_ctx, attn_mode, bias=True):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.n_embd = n_embd\n","        self.dropout = dropout\n","        self.bias = bias\n","        self.local_attn_ctx = local_attn_ctx\n","        self.attn_mode = attn_mode\n","\n","def main():\n","    # Load data\n","    dataloader = load_data(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")\n","    print('Data Loaded')\n","\n","    # Initialize DDP\n","    ddp, master_process, device, seed_offset, ddp_world_size, gradient_accumulation_steps = init_ddp()\n","\n","    # Set seeds and device properties\n","    torch.manual_seed(1337 + seed_offset)\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    ctx = nullcontext()\n","\n","    # Initialize model\n","    model_args = dict(n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, block_size=BLOCK_SIZE,\n","                      bias=BIAS, vocab_size=50000, dropout=DROPOUT,\n","                      local_attn_ctx=LOCAL_ATTN_CTX, attn_mode=ATTN_MODE)\n","    if INIT_FROM == 'scratch':\n","        print(\"Initializing a new model from scratch\")\n","        gptconf = GPTConfig(**model_args)\n","        model = GPT(SparseAttention, gptconf)\n","    elif INIT_FROM == 'resume':\n","        print(f\"Resuming training from {OUT_DIR}\")\n","        ckpt_path = os.path.join(OUT_DIR, 'ckpt.pt')\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","        checkpoint_model_args = checkpoint['model_args']\n","        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","            model_args[k] = checkpoint_model_args[k]\n","        gptconf = GPTConfig(**model_args)\n","        model = GPT(gptconf)\n","        model.load_state_dict(checkpoint['model'])\n","        iter_num = checkpoint['iter_num']\n","        best_val_loss = checkpoint['best_val_loss']\n","    elif INIT_FROM.startswith('gpt2'):\n","        print(f\"Initializing from OpenAI GPT-2 weights: {INIT_FROM}\")\n","        model = GPT.from_pretrained(INIT_FROM, model_args)\n","        if BLOCK_SIZE < model.config.block_size:\n","            model.crop_block_size(BLOCK_SIZE)\n","\n","    model.to(device)\n","\n","    # Initialize optimizer and GradScaler\n","    optimizer = configure_optimizers(model, WEIGHT_DECAY, LEARNING_RATE, (BETA1, BETA2), device)\n","    scaler = GradScaler(enabled=(DTYPE == 'float16' and 'cuda' in device))\n","\n","    # Wrap model in DDP if necessary\n","    if ddp:\n","        model = DDP(model, device_ids=[int(os.environ['LOCAL_RANK'])])\n","\n","    # Training loop\n","    iter_num = 0\n","    best_val_loss = 1e9\n","    t0 = time.time()\n","\n","    while iter_num <= MAX_ITERS:\n","        lr = get_lr(iter_num, LEARNING_RATE, WARMUP_ITERS, LR_DECAY_ITERS, MIN_LR) if DECAY_LR else LEARNING_RATE\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","        if iter_num % EVAL_INTERVAL == 0 and master_process:\n","            losses = estimate_loss(model, dataloader, EVAL_ITERS, device, ctx)\n","            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","            if losses['val'] < best_val_loss or ALWAYS_SAVE_CHECKPOINT:\n","                best_val_loss = losses['val']\n","                if iter_num > 0:\n","                    checkpoint = {\n","                        'model': model.state_dict(),\n","                        'optimizer': optimizer.state_dict(),\n","                        'model_args': model_args,\n","                        'iter_num': iter_num,\n","                        'best_val_loss': best_val_loss\n","                    }\n","                    print(f\"saving checkpoint to {OUT_DIR}\")\n","                    torch.save(checkpoint, os.path.join(OUT_DIR, 'ckpt.pt'))\n","\n","        if iter_num == 0 and EVAL_ONLY:\n","            break\n","\n","        with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n","            total_train_loss = 0.0\n","            for micro_step in range(gradient_accumulation_steps):\n","                batch = next(iter(dataloader))\n","                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","                with ctx:\n","                    logits, loss = model(X, Y)\n","                    loss = loss / gradient_accumulation_steps\n","                scaler.scale(loss).backward()\n","                pbar.update(1)\n","\n","        if GRAD_CLIP != 0.0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        t1 = time.time()\n","        dt = t1 - t0\n","        t0 = t1\n","        if iter_num % LOG_INTERVAL == 0 and master_process:\n","            print(f\"iter {iter_num}: loss {loss.item() * gradient_accumulation_steps:.4f}, time {dt * 1000:.2f}ms\")\n","\n","        iter_num += 1\n","\n","    if ddp:\n","        dist.destroy_process_group()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}
