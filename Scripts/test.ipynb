{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import data utils"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[31361,   294,   282,  ...,   262, 14383,  2612],\n","        [15539, 34248,   544,  ..., 17137,   286,   257]]) tensor([[  294,   282, 21612,  ..., 14383,  2612,   290],\n","        [34248,   544,   318,  ...,   286,   257, 11723]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]])\n"]}],"source":["import pandas as pd\n","from tokenise import TextDataset, pad_sequences\n","from torch.utils.data import DataLoader, Dataset\n","\n","\n","df = pd.read_parquet(\"hf://datasets/gamino/wiki_medical_terms/wiki_medical_terms.parquet\")\n","articles = df.iloc[:, 1]\n","\n","dataset = TextDataset(articles, model=\"gpt2\", seq_length=512)\n","\n","# Create the dataloader\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_sequences)\n","\n","for data in dataloader:\n","    x, y, att = data['input_ids'], data['targets'], data['attention_mask']\n","    print(x,y,att)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# Import models"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from dataclasses import dataclass\n","\n","@dataclass\n","class GPTConfig:\n","    vocab_size = dataset.vocab_size\n","    block_size = 1024\n","    n_layer = 6\n","    n_head = 8\n","    n_embd = 512\n","    dropout = 0.1\n","    bias = False\n","    sparse=False\n","    local_attn_ctx = 32\n","    attn_mode = \"all\"\n","\n","config = GPTConfig()"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer blocks"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from customGPT import Block, GPT"]},{"cell_type":"markdown","metadata":{},"source":["## Attention"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torch \n","from torch import nn\n","import torch.nn.functional as F \n","\n","import math"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["Block(\n","  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (attn): CausalSelfAttention(\n","    (qkv_proj): Linear(in_features=512, out_features=1536, bias=False)\n","    (out_proj): Linear(in_features=512, out_features=512, bias=False)\n","    (attn_dropout): Dropout(p=0.1, inplace=False)\n","    (resid_dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=512, out_features=2048, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Linear(in_features=2048, out_features=512, bias=True)\n","    (3): Dropout(p=0.1, inplace=False)\n","  )\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from attention import CausalSelfAttention\n","\n","class Block(nn.Module):\n","    def __init__(self, att, config):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(config.n_embd, eps=1e-5)\n","        self.attn = att(config)\n","        self.ln2 = nn.LayerNorm(config.n_embd, eps=1e-5)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(config.n_embd, 4 * config.n_embd),\n","            nn.GELU(),\n","            nn.Linear(4 * config.n_embd, config.n_embd),\n","            nn.Dropout(config.dropout),\n","        )\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln1(x))\n","        x = x + self.mlp(self.ln2(x))\n","        return x\n","\n","Block(CausalSelfAttention, config)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["Block(\n","  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (attn): FlashAttention(\n","    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","    (proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=512, out_features=2048, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Linear(in_features=2048, out_features=512, bias=True)\n","    (3): Dropout(p=0.1, inplace=False)\n","  )\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from attention import FlashAttention\n","\n","\n","F_GPT = Block(FlashAttention, config)\n","F_GPT"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["Block(\n","  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (attn): SparseAttention(\n","    (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","    (proj): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  (mlp): Sequential(\n","    (0): Linear(in_features=512, out_features=2048, bias=True)\n","    (1): GELU(approximate='none')\n","    (2): Linear(in_features=2048, out_features=512, bias=True)\n","    (3): Dropout(p=0.1, inplace=False)\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from attention import SparseAttention\n","config.sparse = True\n","config.attn_mode = 'stride'\n","\n","S_GPT = Block(SparseAttention, config)\n","S_GPT"]},{"cell_type":"markdown","metadata":{},"source":["# 5 epoch test"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#training dependencies\n","import numpy as np\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.cuda.amp import GradScaler\n","from contextlib import nullcontext\n","import torch.distributed as dist\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","import os"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens per iteration will be: 40,960\n"]}],"source":["# Default config values designed to train a GPT-2 (124M) on OpenWebText\n","# I/O\n","out_dir = 'out'\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False\n","always_save_checkpoint = True\n","init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n","\n","# Data\n","# data_path = 'data/openwebtext.txt'  # Path to your text file\n","gradient_accumulation_steps = 5 * 8\n","batch_size = 2\n","block_size = 512\n","\n","# Model\n","n_layer = 6  # Reduce the number of layers\n","n_head = 8   # Reduce the number of attention heads\n","n_embd = 512 # Reduce the embedding size\n","dropout = 0.0\n","bias = False\n","\n","# AdamW optimizer\n","learning_rate = 6e-4\n","max_iters = 5\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0\n","\n","# Learning rate decay settings\n","decay_lr = True\n","warmup_iters = 200\n","lr_decay_iters = 1000\n","min_lr = 6e-5\n","\n","# DDP settings\n","backend = 'gloo'\n","\n","# System\n","device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n","dtype = 'float32'  # MPS currently supports only float32\n","compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n","# -----------------------------------------------------------------------------\n","\n","# Collect configuration keys\n","config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","\n","# Various initializations and derived attributes\n","ddp = int(os.environ.get('RANK', -1)) != -1\n","if ddp:\n","    init_process_group(backend=backend)\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n","    torch.mps.set_device(device)\n","    master_process = ddp_rank == 0\n","    seed_offset = ddp_rank\n","    assert gradient_accumulation_steps % ddp_world_size == 0\n","    gradient_accumulation_steps //= ddp_world_size\n","else:\n","    master_process = True\n","    seed_offset = 0\n","    ddp_world_size = 1\n","\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'mps' if 'mps' in device else 'cpu'\n","ptdtype = torch.float32  # Currently, MPS supports only float32\n","ctx = nullcontext()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens per iteration will be: 40,960\n"]}],"source":["# Default config values designed to train a GPT-2 (124M) on OpenWebText\n","# I/O\n","out_dir = 'out'\n","eval_interval = 2000\n","log_interval = 1\n","eval_iters = 200\n","eval_only = False\n","always_save_checkpoint = True\n","init_from = 'scratch'  # 'scratch' or 'resume' or 'gpt2*'\n","\n","# Data\n","# data_path = 'data/openwebtext.txt'  # Path to your text file\n","gradient_accumulation_steps = 5 * 8\n","batch_size = 2\n","block_size = 512\n","\n","#Sparse Attention\n","local_attn_ctx=32\n","attn_mode=\"local\"\n","\n","# Model\n","n_layer = 6  # Reduce the number of layers\n","n_head = 8   # Reduce the number of attention heads\n","n_embd = 512 # Reduce the embedding size\n","dropout = 0.0\n","bias = False\n","\n","# AdamW optimizer\n","learning_rate = 6e-4\n","max_iters = 1000\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.95\n","grad_clip = 1.0\n","\n","# Learning rate decay settings\n","decay_lr = True\n","warmup_iters = 200\n","lr_decay_iters = 1000\n","min_lr = 6e-5\n","\n","# DDP settings\n","backend = 'gloo'\n","\n","# System\n","device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n","dtype = 'float32'  # MPS currently supports only float32\n","compile = False  # Disable compilation for now as PyTorch 2.0 is not yet stable on MPS\n","# -----------------------------------------------------------------------------\n","\n","# Collect configuration keys\n","config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","\n","# Various initializations and derived attributes\n","ddp = int(os.environ.get('RANK', -1)) != -1\n","if ddp:\n","    init_process_group(backend=backend)\n","    ddp_rank = int(os.environ['RANK'])\n","    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n","    ddp_world_size = int(os.environ['WORLD_SIZE'])\n","    device = f'mps:{ddp_local_rank}' if torch.backends.mps.is_available() else f'cuda:{ddp_local_rank}'\n","    torch.mps.set_device(device)\n","    master_process = ddp_rank == 0\n","    seed_offset = ddp_rank\n","    assert gradient_accumulation_steps % ddp_world_size == 0\n","    gradient_accumulation_steps //= ddp_world_size\n","else:\n","    master_process = True\n","    seed_offset = 0\n","    ddp_world_size = 1\n","\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"Tokens per iteration will be: {tokens_per_iter:,}\")\n","\n","if master_process:\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'mps' if 'mps' in device else 'cpu'\n","ptdtype = torch.float32  # Currently, MPS supports only float32\n","ctx = nullcontext()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing a new model from scratch\n","Number of parameters: 19.18M\n"]},{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(512, 512)\n","    (wpe): Embedding(512, 512)\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (h): ModuleList(\n","      (0-5): 6 x Block(\n","        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (attn): SparseAttention(\n","          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","          (proj): Linear(in_features=512, out_features=512, bias=True)\n","        )\n","        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (0): Linear(in_features=512, out_features=2048, bias=True)\n","          (1): GELU(approximate='none')\n","          (2): Linear(in_features=2048, out_features=512, bias=True)\n","          (3): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=512, bias=False)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#configuration\n","class GPTConfig:\n","    def __init__(self, \n","                 vocab_size, \n","                 block_size, \n","                 n_layer, \n","                 n_head, \n","                 n_embd, \n","                 dropout,\n","                 local_attn_ctx,\n","                 attn_mode, \n","                 bias=True):\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.n_embd = n_embd\n","        self.dropout = dropout\n","        self.bias = bias\n","        self.local_attn_ctx = local_attn_ctx\n","        self.attn_mode = attn_mode\n","        \n","        \n","# Initialize iteration number and best validation loss\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","# Model initialization\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=len(dataset.tokens), dropout=dropout, \n","                  local_attn_ctx = local_attn_ctx, attn_mode = attn_mode)\n","\n","if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    gptconf = GPTConfig(**model_args)\n","\n","    #select Attentions\n","    model = GPT(SparseAttention, gptconf)\n","elif init_from == 'resume':\n","    print(f\"Resuming training from {out_dir}\")\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    checkpoint_model_args = checkpoint['model_args']\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = checkpoint_model_args[k]\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k, v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)\n","    iter_num = checkpoint['iter_num']\n","    best_val_loss = checkpoint['best_val_loss']\n","elif init_from.startswith('gpt2'):\n","    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n","    override_args = dict(dropout=dropout)\n","    model = GPT.from_pretrained(init_from, override_args)\n","    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n","        model_args[k] = getattr(model.config, k)\n","\n","if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Initialize a GradScaler\n","scaler = GradScaler(enabled=(dtype == 'float16' and 'cuda' in device))\n","\n","# Configure the optimizer\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","if init_from == 'resume':\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","checkpoint = None  # Free up memory\n","\n","# Wrap model into DDP container if needed\n","if ddp:\n","    print(f\"Starting parallel process with rank {ddp_rank}\")\n","    model = DDP(model, device_ids=[ddp_local_rank])\n","\n","\n","# Function to estimate loss over splits\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        with tqdm(total=eval_iters, desc=f\"Evaluating {split}\") as pbar:\n","            for k in range(eval_iters):\n","                batch = next(iter(dataloader))\n","                X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","                with ctx:\n","                    logits, loss = model(X, Y)\n","                losses[k] = loss.item()\n","                pbar.update(1)\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# Learning rate decay scheduler\n","def get_lr(it):\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    if it > lr_decay_iters:\n","        return min_lr\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","    return min_lr + coeff * (learning_rate - min_lr)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluating train: 100%|██████████| 200/200 [00:21<00:00,  9.28it/s]\n","Evaluating val: 100%|██████████| 200/200 [00:17<00:00, 11.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["step 0: train loss 2.9258, val loss 2.9747\n"]},{"name":"stderr","output_type":"stream","text":["Training step 0: 100%|██████████| 40/40 [00:09<00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 0: loss 3.8065, time 49384.40ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 1: 100%|██████████| 40/40 [00:06<00:00,  6.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 1: loss 4.0380, time 6208.37ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 2: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 2: loss 2.5838, time 6207.54ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 3: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 3: loss 2.4303, time 6219.41ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 4: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 4: loss 2.4450, time 6218.86ms, mfu -100.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 5: 100%|██████████| 40/40 [00:06<00:00,  6.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 5: loss 1.9745, time 6238.10ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 6: 100%|██████████| 40/40 [00:06<00:00,  6.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 6: loss 2.1670, time 6197.10ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 7: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 7: loss 2.0792, time 6204.09ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 8: 100%|██████████| 40/40 [00:06<00:00,  6.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 8: loss 2.1118, time 6225.49ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 9: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 9: loss 1.9406, time 6205.00ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 10: 100%|██████████| 40/40 [00:06<00:00,  6.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 10: loss 1.9479, time 6194.33ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 11: 100%|██████████| 40/40 [00:06<00:00,  6.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 11: loss 1.8528, time 6219.88ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 12: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 12: loss 2.1622, time 6205.05ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 13: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 13: loss 1.9128, time 6201.39ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 14: 100%|██████████| 40/40 [00:06<00:00,  6.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 14: loss 1.0644, time 6225.74ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 15: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 15: loss 1.8839, time 6215.26ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 16: 100%|██████████| 40/40 [00:06<00:00,  6.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 16: loss 1.0318, time 6223.19ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 17: 100%|██████████| 40/40 [00:06<00:00,  6.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 17: loss 1.2050, time 6213.39ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 18: 100%|██████████| 40/40 [00:06<00:00,  6.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 18: loss 1.8623, time 6237.55ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 19: 100%|██████████| 40/40 [00:06<00:00,  6.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 19: loss 1.8851, time 6251.92ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 20: 100%|██████████| 40/40 [00:06<00:00,  6.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 20: loss 1.4758, time 6241.13ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 21: 100%|██████████| 40/40 [00:06<00:00,  6.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 21: loss 1.5570, time 6275.22ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 22: 100%|██████████| 40/40 [00:06<00:00,  6.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["iter 22: loss 0.8982, time 6293.17ms, mfu 0.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training step 23:  12%|█▎        | 5/40 [00:00<00:05,  6.23it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequire_backward_grad_sync \u001b[38;5;241m=\u001b[39m (micro_step \u001b[38;5;241m==\u001b[39m gradient_accumulation_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[0;32m---> 55\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[1;32m     57\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","from tqdm import tqdm\n","\n","\n","# wandb.init(\n","#     # set the wandb project where this run will be logged\n","#     project=project_name, name= run_name, config=config)\n","\n","raw_model = model.module if ddp else model\n","local_iter_num = 0\n","running_mfu = -1.0\n","t0 = time.time()\n","\n","while iter_num <= max_iters:\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","        # if wandb_log:\n","        #     wandb.log({\n","        #         \"iter\": iter_num,\n","        #         \"train_loss\": losses['train'],\n","        #         \"val_loss\": losses['val'],\n","        #         \"lr\": lr,\n","        #         \"mfu\": running_mfu * 100,  # convert to percentage\n","        #     })\n","\n","\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    with tqdm(total=gradient_accumulation_steps, desc=f\"Training step {iter_num}\") as pbar:\n","        total_train_loss = 0.0  # Initialize total training loss\n","        for micro_step in range(gradient_accumulation_steps):\n","            if ddp:\n","                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n","            batch = next(iter(dataloader))\n","            X, Y = batch['input_ids'].to(device), batch['targets'].to(device)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","                loss = loss / gradient_accumulation_steps\n","            scaler.scale(loss).backward()\n","            pbar.update(1)\n","\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5:\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt * 1000:.2f}ms, mfu {running_mfu * 100:.2f}%\")\n","\n","        # if wandb_log:\n","        #     wandb.log({\n","        #         \"iter\": iter_num,\n","        #         \"train_loss\": total_train_loss,\n","        #         \"lr\": lr,\n","        #         \"mfu\": running_mfu * 100,  # convert to percentage\n","        #     })\n","\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","if ddp:\n","    destroy_process_group()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}
